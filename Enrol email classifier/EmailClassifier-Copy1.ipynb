{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#for text processing\n",
    "import re\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "\n",
    "#calculation of time\n",
    "from time import time\n",
    "\n",
    "#model for vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#model for splitting of data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#logistic regression algo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#for k-fold cross validation\n",
    "from sklearn import model_selection\n",
    "\n",
    "#svm algo \n",
    "from sklearn import svm\n",
    "\n",
    "# Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "##pretty print\n",
    "import pprint as pp\n",
    "\n",
    "#SMOTE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file    object\n",
       "body    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('enron_cleaned_sent_emails.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126846, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file  \\\n",
       "0  allen-p/_sent_mail/1.      \n",
       "1  allen-p/_sent_mail/10.     \n",
       "2  allen-p/_sent_mail/100.    \n",
       "3  allen-p/_sent_mail/1000.   \n",
       "4  allen-p/_sent_mail/1001.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \n",
       "0  Here is our forecast\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n  \n",
       "2  test successful.  way to go!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3  Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "4  Let's shoot for Tuesday at 11:45.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file    0\n",
       "body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    \n",
    "    #convert text to lower-case\n",
    "    data['tokenized_text'] = data['body'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
    "\n",
    "    #remove punctuations, unwanted characters\n",
    "    data['tokenized_text_1']= data['tokenized_text'].apply(lambda x: \"\".join([char for char in x if char not in string.punctuation]))\n",
    "\n",
    "    #remove numbers\n",
    "    data['tokenized_text_2']= data['tokenized_text_1'].apply(lambda x: re.sub('[0-9]+', ' ' , x))\n",
    "\n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    data['tokenized_text_3']= data['tokenized_text_2'].apply(lambda x:' '.join(x for x in x.split() if not x in stop))\n",
    "\n",
    "    #lemmatization\n",
    "    data['tokenized_text_4']= data['tokenized_text_3'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "    # remove all single characters\n",
    "    data['tokenized_text_5'] = data['tokenized_text_4'].apply(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "    \n",
    "    #create a final text field to work on\n",
    "    data['final_text'] = data['tokenized_text_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>body</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_text_1</th>\n",
       "      <th>tokenized_text_2</th>\n",
       "      <th>tokenized_text_3</th>\n",
       "      <th>tokenized_text_4</th>\n",
       "      <th>tokenized_text_5</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "      <td>here is our forecast</td>\n",
       "      <td>here is our forecast</td>\n",
       "      <td>here is our forecast</td>\n",
       "      <td>forecast</td>\n",
       "      <td>forecast</td>\n",
       "      <td>forecast</td>\n",
       "      <td>forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n</td>\n",
       "      <td>traveling to have a business meeting takes the fun out of the trip. especially if you have to prepare a presentation. i would suggest holding the business plan meetings here then take a trip without any formal business meetings. i would even try and get some honest opinions on whether a trip is even desired or necessary. as far as the business meetings, i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not. too often the presenter speaks and the others are quiet just waiting for their turn. the meetings might be better if held in a round table discussion format. my suggestion for where to go is austin. play golf and rent a ski boat and jet ski's. flying somewhere takes too much time.</td>\n",
       "      <td>traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time</td>\n",
       "      <td>traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time</td>\n",
       "      <td>traveling business meeting takes fun trip especially prepare presentation would suggest holding business plan meetings take trip without formal business meetings would even try get honest opinions whether trip even desired necessary far business meetings think would productive try stimulate discussions across different groups working often presenter speaks others quiet waiting turn meetings might better held round table discussion format suggestion go austin play golf rent ski boat jet skis flying somewhere takes much time</td>\n",
       "      <td>traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time</td>\n",
       "      <td>traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time</td>\n",
       "      <td>traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "      <td>test successful. way to go!!!</td>\n",
       "      <td>test successful way to go</td>\n",
       "      <td>test successful way to go</td>\n",
       "      <td>test successful way go</td>\n",
       "      <td>test successful way go</td>\n",
       "      <td>test successful way go</td>\n",
       "      <td>test successful way go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip</td>\n",
       "      <td>randy, can you send me a schedule of the salary and level of everyone in the scheduling group. plus your thoughts on any changes that need to be made. (patti s for example) phillip</td>\n",
       "      <td>randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip</td>\n",
       "      <td>randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thoughts changes need made patti example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "      <td>let's shoot for tuesday at 11:45.</td>\n",
       "      <td>lets shoot for tuesday at 1145</td>\n",
       "      <td>lets shoot for tuesday at</td>\n",
       "      <td>lets shoot tuesday</td>\n",
       "      <td>let shoot tuesday</td>\n",
       "      <td>let shoot tuesday</td>\n",
       "      <td>let shoot tuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file  \\\n",
       "0  allen-p/_sent_mail/1.      \n",
       "1  allen-p/_sent_mail/10.     \n",
       "2  allen-p/_sent_mail/100.    \n",
       "3  allen-p/_sent_mail/1000.   \n",
       "4  allen-p/_sent_mail/1001.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
       "0  Here is our forecast\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "1  Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n   \n",
       "2  test successful.  way to go!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "3  Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "4  Let's shoot for Tuesday at 11:45.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          tokenized_text  \\\n",
       "0  here is our forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "1  traveling to have a business meeting takes the fun out of the trip. especially if you have to prepare a presentation. i would suggest holding the business plan meetings here then take a trip without any formal business meetings. i would even try and get some honest opinions on whether a trip is even desired or necessary. as far as the business meetings, i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not. too often the presenter speaks and the others are quiet just waiting for their turn. the meetings might be better if held in a round table discussion format. my suggestion for where to go is austin. play golf and rent a ski boat and jet ski's. flying somewhere takes too much time.   \n",
       "2  test successful. way to go!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3  randy, can you send me a schedule of the salary and level of everyone in the scheduling group. plus your thoughts on any changes that need to be made. (patti s for example) phillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "4  let's shoot for tuesday at 11:45.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tokenized_text_1  \\\n",
       "0  here is our forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1  traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time   \n",
       "2  test successful way to go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4  lets shoot for tuesday at 1145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tokenized_text_2  \\\n",
       "0  here is our forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1  traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time   \n",
       "2  test successful way to go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4  lets shoot for tuesday at                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tokenized_text_3  \\\n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1  traveling business meeting takes fun trip especially prepare presentation would suggest holding business plan meetings take trip without formal business meetings would even try get honest opinions whether trip even desired necessary far business meetings think would productive try stimulate discussions across different groups working often presenter speaks others quiet waiting turn meetings might better held round table discussion format suggestion go austin play golf rent ski boat jet skis flying somewhere takes much time   \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3  randy send schedule salary level everyone scheduling group plus thoughts changes need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4  lets shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         tokenized_text_4  \\\n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time   \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4  let shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         tokenized_text_5  \\\n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time   \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4  let shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               final_text  \n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "1  traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time  \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "3  randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "4  let shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-processing or cleaning data\n",
    "\n",
    "text_preprocessing(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting names of senders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['name'] = df['file'].str.split('/').str[0]\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_names = df['name'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 email senders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mann-k          8926\n",
       "kaminski-v      8644\n",
       "dasovich-j      5366\n",
       "germany-c       5128\n",
       "shackleton-s    4407\n",
       "jones-t         4123\n",
       "bass-e          3030\n",
       "lenhart-m       2759\n",
       "beck-s          2674\n",
       "symes-k         2649\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_sender_names = sender_names.nlargest(10)\n",
    "top10_sender_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_sender_names = sender_names.nlargest(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Email sent by senders')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualization\n",
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=top50_sender_names.index, y=top50_sender_names)\n",
    "plt.xlabel('Senders', fontsize=15)\n",
    "plt.ylabel('Email count', fontsize=15)\n",
    "plt.title('Email sent by senders', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merriss-s         3 \n",
       "phanis-s          4 \n",
       "linder-e          6 \n",
       "meyers-a          11\n",
       "shapiro-r         13\n",
       "motley-m          13\n",
       "bailey-s          14\n",
       "gilbertsmith-d    15\n",
       "king-j            19\n",
       "panus-s           26\n",
       "benson-r          32\n",
       "ermis-f           34\n",
       "holst-k           36\n",
       "saibi-e           39\n",
       "griffith-j        40\n",
       "south-s           42\n",
       "dean-c            44\n",
       "slinger-r         48\n",
       "ring-r            51\n",
       "donohoe-t         52\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smallest20_sender_names = sender_names.nsmallest(20)\n",
    "smallest20_sender_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(data):\n",
    "\n",
    "#     tf=TfidfVectorizer( min_df=10 \n",
    "#                        , max_df=0.7\n",
    "#                        , lowercase=True \n",
    "#                        , ngram_range=(0, 2) )\n",
    "    t0 = time()\n",
    "    tf=TfidfVectorizer( min_df=.01 , max_df=0.50 , ngram_range=(0, 3))\n",
    "    tfidf_dtm = tf.fit_transform(data)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    return tfidf_dtm, tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 106.783s\n",
      "Shape of document term matrix :  (126846, 1562)\n"
     ]
    }
   ],
   "source": [
    "#build a vector from the txt data\n",
    "\n",
    "tfidf_dtm , tf = vectorization(df['final_text'])\n",
    "\n",
    "tfidf_dtm = tfidf_dtm.toarray()\n",
    "print('Shape of document term matrix : ', tfidf_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tfidf'] = list(tfidf_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "pca_dtm = pca.fit_transform(tfidf_dtm)\n",
    "# X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126846, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca'] = list(pca_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>body</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_text_1</th>\n",
       "      <th>tokenized_text_2</th>\n",
       "      <th>tokenized_text_3</th>\n",
       "      <th>tokenized_text_4</th>\n",
       "      <th>tokenized_text_5</th>\n",
       "      <th>final_text</th>\n",
       "      <th>name</th>\n",
       "      <th>pca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "      <td>here is our forecast</td>\n",
       "      <td>here is our forecast</td>\n",
       "      <td>here is our forecast</td>\n",
       "      <td>forecast</td>\n",
       "      <td>forecast</td>\n",
       "      <td>forecast</td>\n",
       "      <td>forecast</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>[0.007739935306755509, -0.05122229538126113, -0.02141562243615556, 0.0037040000314092375, 0.006407401966227062, -0.017127216451015392, -0.05537375478706523, -0.12876582729991548, -0.01702342403881277, -0.008935827509211224, 0.0034910562177183726, -0.03198606929989334, -0.012443045801105663, -0.011724730870031355, -0.0038656150078212347, -0.015200380420638062, -0.061315684516465956, 0.004868301363930693, -0.005666753357673479, 0.004082103203270727, 0.00044563801587900096, 0.013087801006019077, 0.026025655923041693, 0.013126773564932925, 0.012633190190383736, 0.006405479052157535, -0.010089269629107509, 0.010136850068578002, -0.015668129508939685, -0.0028974688471789956, -0.00851883088846224, -0.011735871455991297, -0.006897641056821461, -0.003320085093036331, -0.015721274515374167, -0.008522307295842768, 0.014726222605968509, 0.0023995001110750113, -0.005974096818587393, -0.0040379255608529825, -0.00445143143950546, 0.0022038565594000954, -0.009251374635883162, -0.018050014293015483, -0.005732189192228861, -0.0006559392687304106, 0.006619233848307995, 0.0008456141108019822, 0.002045289435277127, -0.006136567347617895]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n</td>\n",
       "      <td>traveling to have a business meeting takes the fun out of the trip. especially if you have to prepare a presentation. i would suggest holding the business plan meetings here then take a trip without any formal business meetings. i would even try and get some honest opinions on whether a trip is even desired or necessary. as far as the business meetings, i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not. too often the presenter speaks and the others are quiet just waiting for their turn. the meetings might be better if held in a round table discussion format. my suggestion for where to go is austin. play golf and rent a ski boat and jet ski's. flying somewhere takes too much time.</td>\n",
       "      <td>traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time</td>\n",
       "      <td>traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time</td>\n",
       "      <td>traveling business meeting takes fun trip especially prepare presentation would suggest holding business plan meetings take trip without formal business meetings would even try get honest opinions whether trip even desired necessary far business meetings think would productive try stimulate discussions across different groups working often presenter speaks others quiet waiting turn meetings might better held round table discussion format suggestion go austin play golf rent ski boat jet skis flying somewhere takes much time</td>\n",
       "      <td>traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time</td>\n",
       "      <td>traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time</td>\n",
       "      <td>traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>[-0.04504141141469982, -0.06175092951665832, -0.054043713868609546, 0.11346090869991254, -0.08579694725900565, -0.01117679295142871, 0.08766000140387235, 0.03278781983853527, 0.03185563858090285, -0.004304718929786861, -0.11265742839723034, -0.005820638199638244, 0.1378397776453087, -0.029713066321003477, -0.02727525508133003, -0.10552189243520159, -0.08934680188188003, -0.09885219831628705, -0.09006314464152984, -0.01315872076622941, -0.1289068021157012, 0.011355258494605447, 0.026301136094763294, 0.0713920437432062, -0.057085694511519264, -0.04375272205070283, -0.06900599488751008, -0.05339860462416376, -0.10346756349996662, 0.10486459399056737, -0.057103340389331875, 0.016381171364953213, 0.02980468421627691, 0.03560685093732019, 0.03150207569998102, -0.009880675417285045, 0.01747182262491876, -0.05924318531914465, 0.038576990942715564, -0.014201827551783357, -0.03042539287687599, 0.006626317085749685, 0.0041280744634623655, 0.03367250135738957, 0.020295197764522106, -0.039003919989276804, 0.00310264022052389, -0.021519126041864435, 0.01241396192566833, 0.01571933005671787]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "      <td>test successful. way to go!!!</td>\n",
       "      <td>test successful way to go</td>\n",
       "      <td>test successful way to go</td>\n",
       "      <td>test successful way go</td>\n",
       "      <td>test successful way go</td>\n",
       "      <td>test successful way go</td>\n",
       "      <td>test successful way go</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>[-0.015272427996582677, -0.05755698142096804, -0.073491204270112, 0.04195916311129174, -0.03197936334489884, -0.011215271671011339, -0.06630176435745784, -0.09983034779831942, 0.015637129965078354, -0.019277489736172746, 0.037682123173344746, -0.02228639647344874, -0.037641623674000084, -0.014388090640853581, -0.02034256694160216, -0.0164451573029309, -0.05277266243201167, -0.005690321595323805, -0.0010519702275048116, 0.006616367474949107, -0.011297893742362942, -0.007085580326914337, 0.014743580664638324, 0.01596122812243656, 0.01592506002914829, -0.00490785268633803, 0.014001267987122316, 6.032462716221798e-06, -0.06115632327063234, -0.004506753196183111, -0.0152048817723712, -0.009881722275575721, -0.03254160636150424, 0.010794774679572315, -0.0012445285382050634, 0.023895379449622174, 0.004166877185786156, -0.04105259035952678, 0.047198291413816235, 0.003298118890377123, -0.11748365086120556, 0.006430446497038394, 0.0051334477605171885, 0.06087512148329429, -0.023836666555853328, 0.07460425834763004, 0.010746809041447135, 0.04690633660562385, 0.022736025418225305, -0.07011431560297517]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip</td>\n",
       "      <td>randy, can you send me a schedule of the salary and level of everyone in the scheduling group. plus your thoughts on any changes that need to be made. (patti s for example) phillip</td>\n",
       "      <td>randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip</td>\n",
       "      <td>randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thoughts changes need made patti example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip</td>\n",
       "      <td>randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>[-0.011342203128689358, -0.05778541843304511, -0.0179032932805923, -0.004993629289497402, -0.013708869064732879, -0.015049416244710068, 0.014508255561551312, -0.07802832180746228, -0.026975814248252968, -0.00494050518123135, -0.010916189656914604, -0.02075540431686523, -0.025581521348135478, 0.022033694129785997, -0.019567103941596324, -0.02603088941816195, -0.031028122897575405, -0.03196524345732648, -0.015605237325024949, 0.042898639179333294, 0.010848539378128083, -0.03813561312349568, 0.0690252454015097, 0.02085449774855663, -0.05712091038423741, -0.05046262047876034, 0.05548672681252444, -0.028784377966773644, 0.015545139347767486, -0.0036192940825577734, 0.013333104247280918, 0.0342613320397585, 0.002219677100067565, 0.011355705657103364, 0.019663175280036423, 0.022256370842068544, -0.031591433389972674, -0.05770580102469896, 0.00712385771710357, -0.04791249521449956, 0.013774520850596711, -0.007132989716426141, -0.04027608423398859, -0.00872426279998973, 0.0732587808093821, 0.037418798210327696, 0.05984416982456523, -0.08074334187567757, -0.07789213142907515, -0.005384542808239041]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "      <td>let's shoot for tuesday at 11:45.</td>\n",
       "      <td>lets shoot for tuesday at 1145</td>\n",
       "      <td>lets shoot for tuesday at</td>\n",
       "      <td>lets shoot tuesday</td>\n",
       "      <td>let shoot tuesday</td>\n",
       "      <td>let shoot tuesday</td>\n",
       "      <td>let shoot tuesday</td>\n",
       "      <td>allen-p</td>\n",
       "      <td>[-0.037999227547667484, 0.00929454526081194, -0.07561371873512418, 0.02692936909385473, 0.045444734801918144, -0.03486123535292802, -0.052069681301498506, 0.06560642089209169, -0.1396317049418313, 0.13402293557430986, 0.010285094633829782, -0.07618778547332433, -0.08202542453921999, -0.0479726288863059, 0.001617412231231687, -0.00999970691099827, -0.1607253339667089, 0.05135420574007186, -0.0002689258103818748, -0.028349109569455758, 0.003483757392322437, 0.0122847452573171, 0.013691759507737006, 0.025471939547288547, 0.023775026377525873, 0.01705872327978905, -0.038550589182266254, 0.013933280583045389, -0.0003827952955038849, -0.0007562755206504785, -0.023985006482504085, -0.046962997609106584, -0.059042614881688354, 0.023822024557292005, -0.008231875109882845, 0.0021513557176653718, 0.09284615513588616, 0.031196647433166445, -0.05973478347816473, 0.01481052346479474, 0.061864656720271353, 0.120745756624817, 0.1600424214235505, 0.014261949306964512, 0.0888214170802152, -0.024858998419897728, 0.10365020060616922, -0.04211309962905252, -0.06669722752014455, 0.09113885399412897]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file  \\\n",
       "0  allen-p/_sent_mail/1.      \n",
       "1  allen-p/_sent_mail/10.     \n",
       "2  allen-p/_sent_mail/100.    \n",
       "3  allen-p/_sent_mail/1000.   \n",
       "4  allen-p/_sent_mail/1001.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      body  \\\n",
       "0  Here is our forecast\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "1  Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\\n   \n",
       "2  test successful.  way to go!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "3  Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "4  Let's shoot for Tuesday at 11:45.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          tokenized_text  \\\n",
       "0  here is our forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "1  traveling to have a business meeting takes the fun out of the trip. especially if you have to prepare a presentation. i would suggest holding the business plan meetings here then take a trip without any formal business meetings. i would even try and get some honest opinions on whether a trip is even desired or necessary. as far as the business meetings, i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not. too often the presenter speaks and the others are quiet just waiting for their turn. the meetings might be better if held in a round table discussion format. my suggestion for where to go is austin. play golf and rent a ski boat and jet ski's. flying somewhere takes too much time.   \n",
       "2  test successful. way to go!!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3  randy, can you send me a schedule of the salary and level of everyone in the scheduling group. plus your thoughts on any changes that need to be made. (patti s for example) phillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "4  let's shoot for tuesday at 11:45.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tokenized_text_1  \\\n",
       "0  here is our forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1  traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time   \n",
       "2  test successful way to go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4  lets shoot for tuesday at 1145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tokenized_text_2  \\\n",
       "0  here is our forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1  traveling to have a business meeting takes the fun out of the trip especially if you have to prepare a presentation i would suggest holding the business plan meetings here then take a trip without any formal business meetings i would even try and get some honest opinions on whether a trip is even desired or necessary as far as the business meetings i think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not too often the presenter speaks and the others are quiet just waiting for their turn the meetings might be better if held in a round table discussion format my suggestion for where to go is austin play golf and rent a ski boat and jet skis flying somewhere takes too much time   \n",
       "2  test successful way to go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy can you send me a schedule of the salary and level of everyone in the scheduling group plus your thoughts on any changes that need to be made patti s for example phillip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "4  lets shoot for tuesday at                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tokenized_text_3  \\\n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1  traveling business meeting takes fun trip especially prepare presentation would suggest holding business plan meetings take trip without formal business meetings would even try get honest opinions whether trip even desired necessary far business meetings think would productive try stimulate discussions across different groups working often presenter speaks others quiet waiting turn meetings might better held round table discussion format suggestion go austin play golf rent ski boat jet skis flying somewhere takes much time   \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3  randy send schedule salary level everyone scheduling group plus thoughts changes need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4  lets shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         tokenized_text_4  \\\n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time   \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4  let shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         tokenized_text_5  \\\n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time   \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4  let shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               final_text  \\\n",
       "0  forecast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  traveling business meeting take fun trip especially prepare presentation would suggest holding business plan meeting take trip without formal business meeting would even try get honest opinion whether trip even desired necessary far business meeting think would productive try stimulate discussion across different group working often presenter speaks others quiet waiting turn meeting might better held round table discussion format suggestion go austin play golf rent ski boat jet ski flying somewhere take much time   \n",
       "2  test successful way go                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3  randy send schedule salary level everyone scheduling group plus thought change need made patti example phillip                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4  let shoot tuesday                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "\n",
       "      name  \\\n",
       "0  allen-p   \n",
       "1  allen-p   \n",
       "2  allen-p   \n",
       "3  allen-p   \n",
       "4  allen-p   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              pca  \n",
       "0  [0.007739935306755509, -0.05122229538126113, -0.02141562243615556, 0.0037040000314092375, 0.006407401966227062, -0.017127216451015392, -0.05537375478706523, -0.12876582729991548, -0.01702342403881277, -0.008935827509211224, 0.0034910562177183726, -0.03198606929989334, -0.012443045801105663, -0.011724730870031355, -0.0038656150078212347, -0.015200380420638062, -0.061315684516465956, 0.004868301363930693, -0.005666753357673479, 0.004082103203270727, 0.00044563801587900096, 0.013087801006019077, 0.026025655923041693, 0.013126773564932925, 0.012633190190383736, 0.006405479052157535, -0.010089269629107509, 0.010136850068578002, -0.015668129508939685, -0.0028974688471789956, -0.00851883088846224, -0.011735871455991297, -0.006897641056821461, -0.003320085093036331, -0.015721274515374167, -0.008522307295842768, 0.014726222605968509, 0.0023995001110750113, -0.005974096818587393, -0.0040379255608529825, -0.00445143143950546, 0.0022038565594000954, -0.009251374635883162, -0.018050014293015483, -0.005732189192228861, -0.0006559392687304106, 0.006619233848307995, 0.0008456141108019822, 0.002045289435277127, -0.006136567347617895]  \n",
       "1  [-0.04504141141469982, -0.06175092951665832, -0.054043713868609546, 0.11346090869991254, -0.08579694725900565, -0.01117679295142871, 0.08766000140387235, 0.03278781983853527, 0.03185563858090285, -0.004304718929786861, -0.11265742839723034, -0.005820638199638244, 0.1378397776453087, -0.029713066321003477, -0.02727525508133003, -0.10552189243520159, -0.08934680188188003, -0.09885219831628705, -0.09006314464152984, -0.01315872076622941, -0.1289068021157012, 0.011355258494605447, 0.026301136094763294, 0.0713920437432062, -0.057085694511519264, -0.04375272205070283, -0.06900599488751008, -0.05339860462416376, -0.10346756349996662, 0.10486459399056737, -0.057103340389331875, 0.016381171364953213, 0.02980468421627691, 0.03560685093732019, 0.03150207569998102, -0.009880675417285045, 0.01747182262491876, -0.05924318531914465, 0.038576990942715564, -0.014201827551783357, -0.03042539287687599, 0.006626317085749685, 0.0041280744634623655, 0.03367250135738957, 0.020295197764522106, -0.039003919989276804, 0.00310264022052389, -0.021519126041864435, 0.01241396192566833, 0.01571933005671787]                                           \n",
       "2  [-0.015272427996582677, -0.05755698142096804, -0.073491204270112, 0.04195916311129174, -0.03197936334489884, -0.011215271671011339, -0.06630176435745784, -0.09983034779831942, 0.015637129965078354, -0.019277489736172746, 0.037682123173344746, -0.02228639647344874, -0.037641623674000084, -0.014388090640853581, -0.02034256694160216, -0.0164451573029309, -0.05277266243201167, -0.005690321595323805, -0.0010519702275048116, 0.006616367474949107, -0.011297893742362942, -0.007085580326914337, 0.014743580664638324, 0.01596122812243656, 0.01592506002914829, -0.00490785268633803, 0.014001267987122316, 6.032462716221798e-06, -0.06115632327063234, -0.004506753196183111, -0.0152048817723712, -0.009881722275575721, -0.03254160636150424, 0.010794774679572315, -0.0012445285382050634, 0.023895379449622174, 0.004166877185786156, -0.04105259035952678, 0.047198291413816235, 0.003298118890377123, -0.11748365086120556, 0.006430446497038394, 0.0051334477605171885, 0.06087512148329429, -0.023836666555853328, 0.07460425834763004, 0.010746809041447135, 0.04690633660562385, 0.022736025418225305, -0.07011431560297517]                             \n",
       "3  [-0.011342203128689358, -0.05778541843304511, -0.0179032932805923, -0.004993629289497402, -0.013708869064732879, -0.015049416244710068, 0.014508255561551312, -0.07802832180746228, -0.026975814248252968, -0.00494050518123135, -0.010916189656914604, -0.02075540431686523, -0.025581521348135478, 0.022033694129785997, -0.019567103941596324, -0.02603088941816195, -0.031028122897575405, -0.03196524345732648, -0.015605237325024949, 0.042898639179333294, 0.010848539378128083, -0.03813561312349568, 0.0690252454015097, 0.02085449774855663, -0.05712091038423741, -0.05046262047876034, 0.05548672681252444, -0.028784377966773644, 0.015545139347767486, -0.0036192940825577734, 0.013333104247280918, 0.0342613320397585, 0.002219677100067565, 0.011355705657103364, 0.019663175280036423, 0.022256370842068544, -0.031591433389972674, -0.05770580102469896, 0.00712385771710357, -0.04791249521449956, 0.013774520850596711, -0.007132989716426141, -0.04027608423398859, -0.00872426279998973, 0.0732587808093821, 0.037418798210327696, 0.05984416982456523, -0.08074334187567757, -0.07789213142907515, -0.005384542808239041]                               \n",
       "4  [-0.037999227547667484, 0.00929454526081194, -0.07561371873512418, 0.02692936909385473, 0.045444734801918144, -0.03486123535292802, -0.052069681301498506, 0.06560642089209169, -0.1396317049418313, 0.13402293557430986, 0.010285094633829782, -0.07618778547332433, -0.08202542453921999, -0.0479726288863059, 0.001617412231231687, -0.00999970691099827, -0.1607253339667089, 0.05135420574007186, -0.0002689258103818748, -0.028349109569455758, 0.003483757392322437, 0.0122847452573171, 0.013691759507737006, 0.025471939547288547, 0.023775026377525873, 0.01705872327978905, -0.038550589182266254, 0.013933280583045389, -0.0003827952955038849, -0.0007562755206504785, -0.023985006482504085, -0.046962997609106584, -0.059042614881688354, 0.023822024557292005, -0.008231875109882845, 0.0021513557176653718, 0.09284615513588616, 0.031196647433166445, -0.05973478347816473, 0.01481052346479474, 0.061864656720271353, 0.120745756624817, 0.1600424214235505, 0.014261949306964512, 0.0888214170802152, -0.024858998419897728, 0.10365020060616922, -0.04211309962905252, -0.06669722752014455, 0.09113885399412897]                                          "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, label, test_size):\n",
    "    \n",
    "    X = np.array(list(map(np.array, data)))\n",
    "    y = label\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=test_size , random_state=42)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88792, 50)\n",
      "(88792,)\n",
      "(38054, 50)\n",
      "(38054,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test  = split_data(df['pca'] , df['name'] , test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE  \n",
    "Always split into test and train sets BEFORE trying oversampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from imblearn.over_sampling import SVMSMOTE\n",
    "\n",
    "# # print(\"Before OverSampling, counts of label '2': {}\".format(sum(y_train==2)))\n",
    "# # print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "# # print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "# sm = SVMSMOTE(random_state=42)\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "# print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "# print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "# # print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==2)))\n",
    "# # print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "# # print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initiaize supervised learning models\n",
    "\n",
    "def data_sampling(y_train):\n",
    "    \n",
    "    #  Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "    #  samples_100 is the entire training set i.e. len(y_train)\n",
    "    #  samples_10 is 10% of samples_100\n",
    "    #  samples_1 is 1% of samples_100 \n",
    "    \n",
    "    samples = {}\n",
    "    \n",
    "    samples['samples_1']   = int(len(y_train)/100)\n",
    "    samples['samples_10']  = int(len(y_train)*10/100)\n",
    "    samples['samples_100'] = len(y_train)\n",
    "    \n",
    "\n",
    "    print('samples_1   : ', samples['samples_1'])\n",
    "    print('samples_10  : ', samples['samples_10'])\n",
    "    print('samples_100 : ', samples['samples_100'])\n",
    "   \n",
    "    return samples\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict and Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(learner, sample_size, X_train, y_train, X_val, y_val): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_val: features validation set\n",
    "       - y_val: income validation set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train[:sample_size],y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end-start\n",
    "        \n",
    "    # Get the predictions on the validation set(X_val),\n",
    "    start = time() # Get start time\n",
    "    predictions_val = learner.predict(X_val)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "        \n",
    "    #Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_val,predictions_val)\n",
    "    \n",
    "    #classification report for precision, relcall, f1-score\n",
    "#     results['f_val'] = classification_report(y_val, predictions_val, target_names=target_names)\n",
    "    results['f_val'] = classification_report(y_val, predictions_val)\n",
    "               \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples in {}s time with prediction time {}s .\".format(learner.__class__.__name__\n",
    "                                                                                , sample_size,results['train_time'] \n",
    "                                                                                , results['pred_time'] ))\n",
    "    print()\n",
    "    print(\"{} Classification Report \".format(learner.__class__.__name__) )\n",
    "    print()\n",
    "    print(results['f_val'] )\n",
    "    print(\"Accuracy : \", results['acc_test'])\n",
    "    print(\"******************************************************************************************************************\")\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_1   :  887\n",
      "samples_10  :  8879\n",
      "samples_100 :  88792\n",
      "\n",
      "{'samples_1': 887, 'samples_10': 8879, 'samples_100': 88792}\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train_res\n",
    "# y_train = y_train_res\n",
    "\n",
    "samples = {}\n",
    "\n",
    "samples = data_sampling(y_train)\n",
    "\n",
    "print()\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "target_names = list( df['name']\n",
    "                   .unique())\n",
    "#pp.pprint(target_names)\n",
    "pp.pprint(len(target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "clf_A = svm.SVC(kernel='linear' , class_weight = 'balanced')\n",
    "clf_B = LogisticRegression(multi_class='auto' , random_state=42)\n",
    "clf_C = MultinomialNB(alpha=1.0)\n",
    "clf_D = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "\n",
    "print()\n",
    "pp.pprint(clf_A)\n",
    "print()\n",
    "pp.pprint(clf_B)\n",
    "print()\n",
    "pp.pprint(clf_C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR(default parm), RandomForest, Multinomial(doesn't work on -ve values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 887 samples in 0.4341108798980713s time with prediction time 0.07890200614929199s .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.00      0.00      0.00       473\n",
      "       arnold-j       0.00      0.00      0.00       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.79      0.40      0.53       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.28      0.69      0.40       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.00      0.00      0.00       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.00      0.00      0.00       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.55      0.71      0.62      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.00      0.00      0.00       526\n",
      "      derrick-j       0.00      0.00      0.00       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.00      0.00      0.00       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.25      0.00      0.00       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.00      0.00      0.00       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.15      0.77      0.25      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.58      0.06      0.10       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.00      0.00      0.00       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       1.00      0.01      0.02       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.00      0.00      0.00       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.00      0.00      0.00       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.00      0.00      0.00       201\n",
      "        jones-t       0.88      0.44      0.59      1229\n",
      "     kaminski-v       0.32      0.96      0.48      2561\n",
      "         kean-s       0.00      0.00      0.00       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.00      0.00      0.00       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.07      0.00      0.01       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.00      0.00      0.00       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.24      0.23      0.24       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.75      0.61      0.67       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.00      0.00      0.00        45\n",
      "         mann-k       0.21      0.97      0.34      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.00      0.00      0.00       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.00      0.00      0.00       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.00      0.00      0.00       330\n",
      "        nemec-g       0.28      0.25      0.27       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.92      0.81      0.86       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.00      0.00      0.00       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.00      0.00      0.00       451\n",
      "       rogers-b       0.96      0.32      0.48       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.00      0.00      0.00       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.37      0.11      0.16       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.89      0.68      0.77      1256\n",
      "     shankman-j       0.00      0.00      0.00       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.00      0.00      0.00       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.00      0.00      0.00       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.00      0.00      0.00       415\n",
      "      steffes-j       0.00      0.00      0.00       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.00      0.00      0.00       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.98      0.76      0.86       748\n",
      "       taylor-m       0.00      0.00      0.00       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.00      0.00      0.00       284\n",
      "       watson-k       0.00      0.00      0.00       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.00      0.00      0.00       146\n",
      "        whitt-m       0.00      0.00      0.00        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.31     38054\n",
      "      macro avg       0.07      0.06      0.05     38054\n",
      "   weighted avg       0.25      0.31      0.23     38054\n",
      "\n",
      "Accuracy :  0.3147369527513533\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 8879 samples in 8.744186401367188s time with prediction time 0.09408974647521973s .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.50      0.07      0.12       473\n",
      "       arnold-j       0.36      0.42      0.38       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.69      0.67      0.68       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.30      0.77      0.43       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.48      0.20      0.28       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.62      0.02      0.05       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.44      0.82      0.57      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.44      0.48      0.46       526\n",
      "      derrick-j       0.00      0.00      0.00       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.50      0.02      0.04       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.45      0.21      0.29       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.62      0.62      0.62       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.24      0.79      0.37      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.50      0.39      0.44       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.49      0.14      0.22       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.00      0.00      0.00       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.77      0.04      0.07       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.00      0.00      0.00       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.00      0.00      0.00       201\n",
      "        jones-t       0.49      0.72      0.59      1229\n",
      "     kaminski-v       0.63      0.95      0.76      2561\n",
      "         kean-s       0.21      0.03      0.05       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.00      0.00      0.00       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.13      0.22      0.17       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.29      0.05      0.08       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.13      0.72      0.22       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.59      0.74      0.66       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.00      0.00      0.00        45\n",
      "         mann-k       0.80      0.90      0.84      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.35      0.61      0.45       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.00      0.00      0.00       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.00      0.00      0.00       330\n",
      "        nemec-g       0.28      0.44      0.35       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.94      0.82      0.87       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.21      0.06      0.09       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.49      0.21      0.29       451\n",
      "       rogers-b       0.74      0.66      0.70       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       1.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.36      0.29      0.32       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.20      0.45      0.28       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.84      0.88      0.86      1256\n",
      "     shankman-j       1.00      0.00      0.01       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.00      0.00      0.00       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.00      0.00      0.00       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.83      0.71      0.77       415\n",
      "      steffes-j       0.29      0.34      0.32       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.00      0.00      0.00       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.96      0.86      0.91       748\n",
      "       taylor-m       0.33      0.44      0.38       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       1.00      0.00      0.01       284\n",
      "       watson-k       0.64      0.08      0.14       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.00      0.00      0.00       146\n",
      "        whitt-m       0.00      0.00      0.00        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.44     38054\n",
      "      macro avg       0.14      0.11      0.10     38054\n",
      "   weighted avg       0.41      0.44      0.38     38054\n",
      "\n",
      "Accuracy :  0.44334366952225784\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 88792 samples in 224.9145884513855s time with prediction time 0.09777688980102539s .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.50      0.49      0.49       473\n",
      "       arnold-j       0.43      0.57      0.49       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.63      0.71      0.67       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.39      0.79      0.52       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.44      0.51      0.47       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.60      0.13      0.22       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.45      0.50      0.47       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.51      0.80      0.62      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.46      0.63      0.53       526\n",
      "      derrick-j       0.31      0.07      0.12       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.59      0.18      0.28       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.48      0.54      0.51       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.63      0.75      0.69       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.13      0.04      0.06       168\n",
      "      germany-c       0.37      0.73      0.49      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.48      0.46      0.47       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.50      0.40      0.44       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.57      0.21      0.31       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.18      0.06      0.09       190\n",
      "        heard-m       0.70      0.47      0.56       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.39      0.27      0.32       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.19      0.02      0.03       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.50      0.04      0.07       201\n",
      "        jones-t       0.55      0.74      0.63      1229\n",
      "     kaminski-v       0.83      0.94      0.88      2561\n",
      "         kean-s       0.25      0.18      0.21       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.74      0.19      0.31       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.30      0.37      0.34       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.19      0.11      0.14       547\n",
      "          lay-k       0.50      0.02      0.03       172\n",
      "      lenhart-m       0.16      0.71      0.26       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.72      0.75      0.74       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.93      0.31      0.47        45\n",
      "         mann-k       0.86      0.90      0.88      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.47      0.65      0.55       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.10      0.03      0.05       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.34      0.21      0.26       330\n",
      "        nemec-g       0.33      0.48      0.39       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.13      0.04      0.06       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.93      0.86      0.89       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.27      0.27      0.27       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.38      0.05      0.08       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.32      0.41      0.36       451\n",
      "       rogers-b       0.72      0.74      0.73       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.32      0.07      0.11       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.34      0.42      0.38       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.39      0.19      0.26       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.28      0.48      0.36       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.87      0.89      0.88      1256\n",
      "     shankman-j       0.60      0.11      0.19       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.41      0.04      0.08       208\n",
      "     skilling-j       0.35      0.05      0.09       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.50      0.00      0.01       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.80      0.79      0.80       415\n",
      "      steffes-j       0.34      0.59      0.44       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.64      0.11      0.18       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.92      0.91      0.92       748\n",
      "       taylor-m       0.39      0.49      0.44       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.38      0.30      0.34       284\n",
      "       watson-k       0.33      0.42      0.37       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.53      0.07      0.12       146\n",
      "        whitt-m       0.33      0.01      0.02        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.50      0.03      0.06       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.51     38054\n",
      "      macro avg       0.20      0.16      0.16     38054\n",
      "   weighted avg       0.47      0.51      0.46     38054\n",
      "\n",
      "Accuracy :  0.509013507121459\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier trained on 887 samples in 0.4551737308502197s time with prediction time 4.871944427490234s .\n",
      "\n",
      "RandomForestClassifier Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.00      0.00      0.00       473\n",
      "       arnold-j       0.00      0.00      0.00       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.79      0.03      0.06       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.00      0.00      0.00       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.00      0.00      0.00       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.00      0.00      0.00       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.35      0.52      0.42      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.00      0.00      0.00       526\n",
      "      derrick-j       0.00      0.00      0.00       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.00      0.00      0.00       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.00      0.00      0.00       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.00      0.00      0.00       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.14      0.39      0.20      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.00      0.00      0.00       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.00      0.00      0.00       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.00      0.00      0.00       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.00      0.00      0.00       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.00      0.00      0.00       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.00      0.00      0.00       201\n",
      "        jones-t       0.98      0.09      0.17      1229\n",
      "     kaminski-v       0.37      0.89      0.52      2561\n",
      "         kean-s       0.00      0.00      0.00       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.00      0.00      0.00       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.00      0.00      0.00       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.00      0.00      0.00       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.00      0.00      0.00       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.54      0.40      0.46       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.00      0.00      0.00        45\n",
      "         mann-k       0.12      0.97      0.21      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.00      0.00      0.00       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.00      0.00      0.00       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.00      0.00      0.00       330\n",
      "        nemec-g       0.00      0.00      0.00       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.96      0.47      0.63       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.00      0.00      0.00       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.00      0.00      0.00       451\n",
      "       rogers-b       0.00      0.00      0.00       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.00      0.00      0.00       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.00      0.00      0.00       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.96      0.20      0.33      1256\n",
      "     shankman-j       0.00      0.00      0.00       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.00      0.00      0.00       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.00      0.00      0.00       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.00      0.00      0.00       415\n",
      "      steffes-j       0.00      0.00      0.00       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.00      0.00      0.00       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.85      0.70      0.76       748\n",
      "       taylor-m       0.00      0.00      0.00       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.00      0.00      0.00       284\n",
      "       watson-k       0.00      0.00      0.00       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.00      0.00      0.00       146\n",
      "        whitt-m       0.00      0.00      0.00        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.21     38054\n",
      "      macro avg       0.04      0.03      0.03     38054\n",
      "   weighted avg       0.18      0.21      0.13     38054\n",
      "\n",
      "Accuracy :  0.20804646029326745\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier trained on 8879 samples in 4.41886305809021s time with prediction time 6.694739818572998s .\n",
      "\n",
      "RandomForestClassifier Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.00      0.00      0.00       473\n",
      "       arnold-j       0.00      0.00      0.00       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.00      0.00      0.00       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.00      0.00      0.00       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.00      0.00      0.00       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.00      0.00      0.00       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.38      0.59      0.46      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.00      0.00      0.00       526\n",
      "      derrick-j       0.00      0.00      0.00       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.00      0.00      0.00       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.00      0.00      0.00       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.00      0.00      0.00       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.19      0.30      0.23      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.00      0.00      0.00       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.00      0.00      0.00       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.00      0.00      0.00       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.00      0.00      0.00       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.00      0.00      0.00       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.00      0.00      0.00       201\n",
      "        jones-t       0.95      0.19      0.32      1229\n",
      "     kaminski-v       0.39      0.90      0.54      2561\n",
      "         kean-s       0.00      0.00      0.00       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.00      0.00      0.00       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.00      0.00      0.00       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.00      0.00      0.00       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.00      0.00      0.00       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.00      0.00      0.00       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.00      0.00      0.00        45\n",
      "         mann-k       0.11      0.97      0.20      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.00      0.00      0.00       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.00      0.00      0.00       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.00      0.00      0.00       330\n",
      "        nemec-g       0.00      0.00      0.00       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.98      0.69      0.81       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.00      0.00      0.00       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.00      0.00      0.00       451\n",
      "       rogers-b       0.00      0.00      0.00       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.00      0.00      0.00       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.00      0.00      0.00       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.74      0.65      0.69      1256\n",
      "     shankman-j       0.00      0.00      0.00       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.00      0.00      0.00       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.00      0.00      0.00       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.93      0.21      0.35       415\n",
      "      steffes-j       0.00      0.00      0.00       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.00      0.00      0.00       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.86      0.68      0.76       748\n",
      "       taylor-m       0.00      0.00      0.00       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.00      0.00      0.00       284\n",
      "       watson-k       0.00      0.00      0.00       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.00      0.00      0.00       146\n",
      "        whitt-m       0.00      0.00      0.00        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.22     38054\n",
      "      macro avg       0.04      0.03      0.03     38054\n",
      "   weighted avg       0.16      0.22      0.15     38054\n",
      "\n",
      "Accuracy :  0.2238660850370526\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier trained on 88792 samples in 44.505295515060425s time with prediction time 8.062323570251465s .\n",
      "\n",
      "RandomForestClassifier Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.00      0.00      0.00       473\n",
      "       arnold-j       0.00      0.00      0.00       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.00      0.00      0.00       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.00      0.00      0.00       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.00      0.00      0.00       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.00      0.00      0.00       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.32      0.61      0.42      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.00      0.00      0.00       526\n",
      "      derrick-j       0.00      0.00      0.00       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.00      0.00      0.00       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.00      0.00      0.00       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.00      0.00      0.00       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.60      0.21      0.31      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.00      0.00      0.00       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.00      0.00      0.00       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.00      0.00      0.00       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.00      0.00      0.00       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.00      0.00      0.00       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.00      0.00      0.00       201\n",
      "        jones-t       0.90      0.19      0.32      1229\n",
      "     kaminski-v       0.42      0.88      0.57      2561\n",
      "         kean-s       0.00      0.00      0.00       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.00      0.00      0.00       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.00      0.00      0.00       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.00      0.00      0.00       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.00      0.00      0.00       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.00      0.00      0.00       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.00      0.00      0.00        45\n",
      "         mann-k       0.10      0.97      0.18      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.00      0.00      0.00       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.00      0.00      0.00       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.00      0.00      0.00       330\n",
      "        nemec-g       0.00      0.00      0.00       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.98      0.73      0.83       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.00      0.00      0.00       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.00      0.00      0.00       451\n",
      "       rogers-b       0.00      0.00      0.00       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.00      0.00      0.00       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.00      0.00      0.00       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.69      0.69      0.69      1256\n",
      "     shankman-j       0.00      0.00      0.00       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.00      0.00      0.00       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.00      0.00      0.00       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.00      0.00      0.00       415\n",
      "      steffes-j       0.00      0.00      0.00       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.00      0.00      0.00       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.88      0.68      0.77       748\n",
      "       taylor-m       0.00      0.00      0.00       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.00      0.00      0.00       284\n",
      "       watson-k       0.00      0.00      0.00       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.00      0.00      0.00       146\n",
      "        whitt-m       0.00      0.00      0.00        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.22     38054\n",
      "      macro avg       0.03      0.03      0.03     38054\n",
      "   weighted avg       0.16      0.22      0.15     38054\n",
      "\n",
      "Accuracy :  0.22021338098491616\n",
      "******************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for clf in [clf_B,clf_D]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    \n",
    "    for i in samples.values():\n",
    "        results[clf_name][i] = train_predict(clf, i, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression after Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 887 samples in 0.46878862380981445s time with prediction time 0.09371209144592285s .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.00      0.00      0.00       473\n",
      "       arnold-j       0.00      0.00      0.00       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.79      0.40      0.53       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.28      0.69      0.40       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.00      0.00      0.00       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.00      0.00      0.00       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.55      0.71      0.62      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.00      0.00      0.00       526\n",
      "      derrick-j       0.00      0.00      0.00       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.00      0.00      0.00       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.25      0.00      0.00       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.00      0.00      0.00       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.15      0.77      0.25      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.58      0.06      0.10       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.00      0.00      0.00       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       1.00      0.01      0.02       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.00      0.00      0.00       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.00      0.00      0.00       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.00      0.00      0.00       201\n",
      "        jones-t       0.88      0.44      0.59      1229\n",
      "     kaminski-v       0.32      0.96      0.48      2561\n",
      "         kean-s       0.00      0.00      0.00       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.00      0.00      0.00       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.07      0.00      0.01       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.00      0.00      0.00       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.24      0.23      0.24       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.75      0.61      0.67       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.00      0.00      0.00        45\n",
      "         mann-k       0.21      0.97      0.34      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.00      0.00      0.00       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.00      0.00      0.00       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.00      0.00      0.00       330\n",
      "        nemec-g       0.28      0.25      0.27       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.92      0.81      0.86       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.00      0.00      0.00       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.00      0.00      0.00       451\n",
      "       rogers-b       0.96      0.32      0.48       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.00      0.00      0.00       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.37      0.11      0.16       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.89      0.68      0.77      1256\n",
      "     shankman-j       0.00      0.00      0.00       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.00      0.00      0.00       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.00      0.00      0.00       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.00      0.00      0.00       415\n",
      "      steffes-j       0.00      0.00      0.00       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.00      0.00      0.00       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.98      0.76      0.86       748\n",
      "       taylor-m       0.00      0.00      0.00       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.00      0.00      0.00       284\n",
      "       watson-k       0.00      0.00      0.00       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.00      0.00      0.00       146\n",
      "        whitt-m       0.00      0.00      0.00        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.31     38054\n",
      "      macro avg       0.07      0.06      0.05     38054\n",
      "   weighted avg       0.25      0.31      0.23     38054\n",
      "\n",
      "Accuracy :  0.3147369527513533\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 8879 samples in 16.001776933670044s time with prediction time 0.11860179901123047s .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.50      0.07      0.12       473\n",
      "       arnold-j       0.36      0.42      0.38       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.69      0.67      0.68       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.30      0.77      0.43       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.48      0.20      0.28       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.62      0.02      0.05       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.44      0.82      0.57      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.44      0.48      0.46       526\n",
      "      derrick-j       0.00      0.00      0.00       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.50      0.02      0.04       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.45      0.21      0.29       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.62      0.62      0.62       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.24      0.79      0.37      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.50      0.39      0.44       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.49      0.14      0.22       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.00      0.00      0.00       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.77      0.04      0.07       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.00      0.00      0.00       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.00      0.00      0.00       201\n",
      "        jones-t       0.49      0.72      0.59      1229\n",
      "     kaminski-v       0.63      0.95      0.76      2561\n",
      "         kean-s       0.21      0.03      0.05       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.00      0.00      0.00       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.13      0.22      0.17       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.29      0.05      0.08       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.13      0.72      0.22       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.59      0.74      0.66       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.00      0.00      0.00        45\n",
      "         mann-k       0.80      0.90      0.84      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.35      0.61      0.45       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.00      0.00      0.00       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.00      0.00      0.00       330\n",
      "        nemec-g       0.28      0.44      0.35       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.94      0.82      0.87       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.21      0.06      0.09       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.49      0.21      0.29       451\n",
      "       rogers-b       0.74      0.66      0.70       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       1.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.36      0.29      0.32       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.20      0.45      0.28       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.84      0.88      0.86      1256\n",
      "     shankman-j       1.00      0.00      0.01       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.00      0.00      0.00       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.00      0.00      0.00       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.83      0.71      0.77       415\n",
      "      steffes-j       0.29      0.34      0.32       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.00      0.00      0.00       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.96      0.86      0.91       748\n",
      "       taylor-m       0.33      0.44      0.38       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       1.00      0.00      0.01       284\n",
      "       watson-k       0.64      0.08      0.14       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.00      0.00      0.00       146\n",
      "        whitt-m       0.00      0.00      0.00        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.44     38054\n",
      "      macro avg       0.14      0.11      0.10     38054\n",
      "   weighted avg       0.41      0.44      0.38     38054\n",
      "\n",
      "Accuracy :  0.44334366952225784\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 88792 samples in 237.06599068641663s time with prediction time 0.09773683547973633s .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.50      0.49      0.49       473\n",
      "       arnold-j       0.43      0.57      0.49       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.63      0.71      0.67       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.39      0.79      0.52       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.44      0.51      0.47       293\n",
      "      brawner-s       0.00      0.00      0.00       109\n",
      "          buy-r       0.60      0.13      0.22       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.45      0.50      0.47       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.00      0.00      0.00       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.51      0.80      0.62      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.46      0.63      0.53       526\n",
      "      derrick-j       0.31      0.07      0.12       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.00      0.00        15\n",
      "      dorland-c       0.59      0.18      0.28       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.48      0.54      0.51       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.63      0.75      0.69       645\n",
      "         gang-l       0.00      0.00      0.00        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.13      0.04      0.06       168\n",
      "      germany-c       0.37      0.73      0.49      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.48      0.46      0.47       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.50      0.40      0.44       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.57      0.21      0.31       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.18      0.06      0.09       190\n",
      "        heard-m       0.70      0.47      0.56       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.39      0.27      0.32       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.19      0.02      0.03       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.50      0.04      0.07       201\n",
      "        jones-t       0.55      0.74      0.63      1229\n",
      "     kaminski-v       0.83      0.94      0.88      2561\n",
      "         kean-s       0.25      0.18      0.21       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.74      0.19      0.31       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.30      0.37      0.34       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.19      0.11      0.14       547\n",
      "          lay-k       0.50      0.02      0.03       172\n",
      "      lenhart-m       0.16      0.71      0.26       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.72      0.75      0.74       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.93      0.31      0.47        45\n",
      "         mann-k       0.86      0.90      0.88      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.47      0.65      0.55       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.10      0.03      0.05       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.00      0.00      0.00       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.34      0.21      0.26       330\n",
      "        nemec-g       0.33      0.48      0.39       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.13      0.04      0.06       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.93      0.86      0.89       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.27      0.27      0.27       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.00      0.00      0.00       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.38      0.05      0.08       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.32      0.41      0.36       451\n",
      "       rogers-b       0.72      0.74      0.73       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.32      0.07      0.11       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.34      0.42      0.38       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.39      0.19      0.26       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.28      0.48      0.36       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.87      0.89      0.88      1256\n",
      "     shankman-j       0.60      0.11      0.19       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.41      0.04      0.08       208\n",
      "     skilling-j       0.35      0.05      0.09       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.50      0.00      0.01       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.80      0.79      0.80       415\n",
      "      steffes-j       0.34      0.59      0.44       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.64      0.11      0.18       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.00      0.00      0.00       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       0.92      0.91      0.92       748\n",
      "       taylor-m       0.39      0.49      0.44       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.38      0.30      0.34       284\n",
      "       watson-k       0.33      0.42      0.37       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.53      0.07      0.12       146\n",
      "        whitt-m       0.33      0.01      0.02        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.50      0.03      0.06       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.51     38054\n",
      "      macro avg       0.20      0.16      0.16     38054\n",
      "   weighted avg       0.47      0.51      0.46     38054\n",
      "\n",
      "Accuracy :  0.509013507121459\n",
      "******************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "logreg2=LogisticRegression(C=1,penalty=\"l2\")\n",
    "\n",
    "results = {}\n",
    "for clf in [logreg2]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    \n",
    "    for i in samples.values():\n",
    "        results[clf_name][i] = train_predict(clf, i, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Produces good results (better than others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC trained on 887 samples in 0.34365105628967285s time with prediction time 10.651819944381714s .\n",
      "\n",
      "SVC Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.00      0.00      0.00       473\n",
      "       arnold-j       0.00      0.00      0.00       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.00      0.00        16\n",
      "       bailey-s       0.00      0.00      0.00         4\n",
      "         bass-e       0.92      0.25      0.39       893\n",
      "     baughman-d       0.00      0.00      0.00        32\n",
      "         beck-s       0.11      0.83      0.20       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.00      0.00      0.00       293\n",
      "      brawner-s       0.02      0.02      0.02       109\n",
      "          buy-r       0.00      0.00      0.00       233\n",
      "     campbell-l       0.00      0.00      0.00       203\n",
      "       carson-m       0.00      0.00      0.00       135\n",
      "         cash-m       0.50      0.00      0.01       325\n",
      "    causholli-m       0.00      0.00      0.00        69\n",
      "       corman-s       0.03      0.01      0.01       185\n",
      "     crandell-s       0.00      0.00      0.00        47\n",
      "       cuilla-m       0.00      0.00      0.00        53\n",
      "     dasovich-j       0.85      0.25      0.38      1609\n",
      "        davis-d       0.00      0.00      0.00       114\n",
      "         dean-c       0.00      0.00      0.00        13\n",
      "     delainey-d       0.00      0.00      0.00       526\n",
      "      derrick-j       0.03      0.02      0.03       204\n",
      "      dickson-s       0.00      0.00      0.00        56\n",
      "       donoho-l       0.00      0.00      0.00        64\n",
      "      donohoe-t       0.00      0.07      0.01        15\n",
      "      dorland-c       0.50      0.06      0.10       323\n",
      "        ermis-f       0.00      0.00      0.00        13\n",
      "       farmer-d       0.71      0.04      0.07       484\n",
      "      fischer-m       0.00      0.00      0.00        34\n",
      "       forney-j       0.00      0.00      0.00       115\n",
      "       fossum-d       0.32      0.40      0.35       645\n",
      "         gang-l       0.01      0.09      0.01        32\n",
      "          gay-r       0.00      0.00      0.00       181\n",
      "     geaccone-t       0.00      0.00      0.00       168\n",
      "      germany-c       0.92      0.02      0.04      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.63      0.08      0.14       580\n",
      "     griffith-j       0.00      0.00      0.00        13\n",
      "      grigsby-m       0.23      0.01      0.02       260\n",
      "       guzman-m       0.00      0.00      0.00        96\n",
      "     haedicke-m       0.38      0.15      0.22       358\n",
      "         hain-m       0.00      0.00      0.00       123\n",
      "     hayslett-r       0.00      0.00      0.00       190\n",
      "        heard-m       0.06      0.42      0.10       255\n",
      "  hendrickson-s       0.00      0.00      0.00        33\n",
      "    hernandez-j       0.00      0.00      0.00       220\n",
      "        hodge-j       0.00      0.00      0.00        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.05      0.03      0.04       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.22      0.05      0.08       201\n",
      "        jones-t       0.99      0.23      0.37      1229\n",
      "     kaminski-v       0.99      0.46      0.63      2561\n",
      "         kean-s       0.00      0.00      0.00       544\n",
      "       keavey-p       0.00      0.00      0.00        58\n",
      "       keiser-k       0.18      0.05      0.08       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.00      0.00      0.00       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.00      0.00      0.00       547\n",
      "          lay-k       0.00      0.00      0.00       172\n",
      "      lenhart-m       0.00      0.00      0.00       828\n",
      "        lewis-a       0.00      0.00      0.00        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.00      0.00      0.00        96\n",
      "        lokey-t       0.00      0.00      0.00        44\n",
      "         love-p       0.86      0.19      0.31       676\n",
      "        lucci-p       0.00      0.00      0.00        72\n",
      "        maggi-m       0.17      0.33      0.23        45\n",
      "         mann-k       1.00      0.22      0.36      2763\n",
      "       martin-t       0.00      0.00      0.00        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.00      0.00      0.00        47\n",
      "    mcconnell-m       0.07      0.24      0.11       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.00      0.00      0.00        87\n",
      "   mclaughlin-e       0.13      0.02      0.04       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.02      0.47      0.04       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.03      0.28      0.06       330\n",
      "        nemec-g       1.00      0.00      0.01       621\n",
      "        panus-s       0.00      0.00      0.00        11\n",
      "        parks-j       0.00      0.00      0.00       164\n",
      "      pereira-s       0.00      0.00      0.00        99\n",
      "  perlingiere-d       0.96      0.78      0.86       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.02      0.62      0.03       266\n",
      "       quenet-j       0.00      0.00      0.00        43\n",
      "      quigley-d       0.03      0.25      0.05       143\n",
      "         rapp-b       0.00      0.00      0.00        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.00      0.00      0.00       105\n",
      "         ring-a       0.00      0.00      0.00        84\n",
      "         ring-r       0.00      0.00      0.00        17\n",
      "     rodrique-r       0.00      0.00      0.00       451\n",
      "       rogers-b       0.93      0.21      0.34       550\n",
      "     ruscitti-k       0.00      0.00      0.00       122\n",
      "        sager-e       0.00      0.00      0.00       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.00      0.00      0.00        35\n",
      "      sanchez-m       0.00      0.00      0.00        32\n",
      "      sanders-r       0.64      0.01      0.02       619\n",
      "     scholtes-d       0.00      0.00      0.00        80\n",
      "  schoolcraft-d       0.00      0.00      0.00       167\n",
      "    schwieger-j       0.00      0.00      0.00        52\n",
      "        scott-s       0.00      0.00      0.00       779\n",
      "    semperger-c       0.00      0.00      0.00        80\n",
      "   shackleton-s       0.99      0.36      0.52      1256\n",
      "     shankman-j       0.05      0.03      0.04       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.23      0.01      0.03       208\n",
      "     skilling-j       0.00      0.00      0.00       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.03      0.02      0.03       232\n",
      "      solberg-g       0.00      0.00      0.00        20\n",
      "        south-s       0.00      0.00      0.00        13\n",
      "        staab-t       0.00      0.00      0.00        23\n",
      "      stclair-c       0.81      0.59      0.68       415\n",
      "      steffes-j       0.24      0.29      0.26       392\n",
      " stepenovitch-j       0.00      0.00      0.00        19\n",
      "      stokley-c       0.37      0.06      0.10       170\n",
      "       storey-g       0.00      0.00      0.00        37\n",
      "        sturm-f       0.01      0.01      0.01       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       1.00      0.70      0.82       748\n",
      "       taylor-m       0.00      0.00      0.00       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.00      0.00      0.00        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.00      0.00      0.00       284\n",
      "       watson-k       0.06      0.00      0.01       277\n",
      "       weldon-c       0.00      0.00      0.00       136\n",
      "      whalley-g       0.00      0.00      0.00        33\n",
      "      whalley-l       0.00      0.00      0.00       119\n",
      "        white-s       0.03      0.03      0.03       146\n",
      "        whitt-m       0.00      0.01      0.01        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.00      0.00      0.00       157\n",
      "        wolfe-j       0.00      0.00      0.00        24\n",
      "       ybarbo-p       0.00      0.00      0.00        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.00      0.00      0.00       105\n",
      "\n",
      "       accuracy                           0.17     38054\n",
      "      macro avg       0.12      0.06      0.06     38054\n",
      "   weighted avg       0.45      0.17      0.20     38054\n",
      "\n",
      "Accuracy :  0.17393703684238188\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC trained on 8879 samples in 19.50966238975525s time with prediction time 109.16343259811401s .\n",
      "\n",
      "SVC Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.66      0.29      0.40       473\n",
      "       arnold-j       0.44      0.20      0.27       727\n",
      "        arora-h       0.00      0.00      0.00        20\n",
      "       badeer-r       0.00      0.06      0.01        16\n",
      "       bailey-s       0.05      0.50      0.08         4\n",
      "         bass-e       0.87      0.45      0.59       893\n",
      "     baughman-d       0.00      0.03      0.01        32\n",
      "         beck-s       0.65      0.52      0.58       801\n",
      "       benson-r       0.00      0.00      0.00         8\n",
      "        blair-l       0.47      0.29      0.36       293\n",
      "      brawner-s       0.08      0.18      0.11       109\n",
      "          buy-r       0.20      0.17      0.18       233\n",
      "     campbell-l       0.07      0.23      0.10       203\n",
      "       carson-m       0.02      0.05      0.03       135\n",
      "         cash-m       0.45      0.21      0.29       325\n",
      "    causholli-m       0.06      0.13      0.08        69\n",
      "       corman-s       0.17      0.05      0.08       185\n",
      "     crandell-s       0.01      0.04      0.02        47\n",
      "       cuilla-m       0.03      0.04      0.03        53\n",
      "     dasovich-j       0.84      0.45      0.59      1609\n",
      "        davis-d       0.13      0.11      0.12       114\n",
      "         dean-c       0.00      0.08      0.01        13\n",
      "     delainey-d       0.54      0.44      0.49       526\n",
      "      derrick-j       0.10      0.09      0.09       204\n",
      "      dickson-s       0.02      0.29      0.04        56\n",
      "       donoho-l       0.01      0.02      0.01        64\n",
      "      donohoe-t       0.00      0.13      0.01        15\n",
      "      dorland-c       0.30      0.39      0.34       323\n",
      "        ermis-f       0.02      0.08      0.03        13\n",
      "       farmer-d       0.55      0.33      0.41       484\n",
      "      fischer-m       0.09      0.12      0.10        34\n",
      "       forney-j       0.06      0.02      0.03       115\n",
      "       fossum-d       0.85      0.57      0.68       645\n",
      "         gang-l       0.01      0.09      0.01        32\n",
      "          gay-r       0.02      0.03      0.02       181\n",
      "     geaccone-t       0.09      0.06      0.07       168\n",
      "      germany-c       0.94      0.17      0.28      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.62      0.18      0.28       580\n",
      "     griffith-j       0.01      0.08      0.01        13\n",
      "      grigsby-m       0.41      0.59      0.49       260\n",
      "       guzman-m       0.03      0.17      0.05        96\n",
      "     haedicke-m       0.33      0.50      0.40       358\n",
      "         hain-m       0.10      0.41      0.16       123\n",
      "     hayslett-r       0.18      0.09      0.12       190\n",
      "        heard-m       0.43      0.38      0.40       255\n",
      "  hendrickson-s       0.03      0.18      0.05        33\n",
      "    hernandez-j       0.23      0.30      0.26       220\n",
      "        hodge-j       0.09      0.12      0.11        49\n",
      "        holst-k       0.00      0.00      0.00        12\n",
      "       horton-s       0.07      0.01      0.02       254\n",
      "        hyatt-k       0.00      0.00      0.00       101\n",
      "         hyvl-d       0.16      0.18      0.17       201\n",
      "        jones-t       0.96      0.41      0.57      1229\n",
      "     kaminski-v       0.99      0.72      0.83      2561\n",
      "         kean-s       0.25      0.05      0.08       544\n",
      "       keavey-p       0.01      0.02      0.01        58\n",
      "       keiser-k       0.19      0.39      0.26       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.35      0.08      0.13       327\n",
      "   kuykendall-t       0.00      0.00      0.00       159\n",
      "     lavorato-j       0.29      0.08      0.13       547\n",
      "          lay-k       0.11      0.28      0.16       172\n",
      "      lenhart-m       0.38      0.02      0.04       828\n",
      "        lewis-a       0.01      0.08      0.02        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.11      0.16      0.13        96\n",
      "        lokey-t       0.02      0.16      0.04        44\n",
      "         love-p       0.76      0.49      0.59       676\n",
      "        lucci-p       0.03      0.03      0.03        72\n",
      "        maggi-m       0.67      0.40      0.50        45\n",
      "         mann-k       0.98      0.68      0.80      2763\n",
      "       martin-t       0.06      0.16      0.09        68\n",
      "          may-l       0.00      0.00      0.00        35\n",
      "      mccarty-d       0.01      0.02      0.01        47\n",
      "    mcconnell-m       0.62      0.35      0.45       484\n",
      "        mckay-b       0.00      0.00      0.00        49\n",
      "        mckay-j       0.03      0.01      0.02        87\n",
      "   mclaughlin-e       0.20      0.08      0.11       212\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.01      0.00      0.01       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.29      0.19      0.23       330\n",
      "        nemec-g       0.29      0.08      0.12       621\n",
      "        panus-s       0.10      0.09      0.10        11\n",
      "        parks-j       0.19      0.02      0.03       164\n",
      "      pereira-s       0.16      0.19      0.18        99\n",
      "  perlingiere-d       0.96      0.80      0.87       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.00      0.00      0.00        31\n",
      "       presto-k       0.39      0.09      0.15       266\n",
      "       quenet-j       0.00      0.21      0.01        43\n",
      "      quigley-d       0.09      0.02      0.03       143\n",
      "         rapp-b       0.01      0.04      0.02        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.33      0.01      0.02       105\n",
      "         ring-a       0.05      0.14      0.07        84\n",
      "         ring-r       0.04      0.18      0.06        17\n",
      "     rodrique-r       0.26      0.16      0.20       451\n",
      "       rogers-b       0.91      0.59      0.72       550\n",
      "     ruscitti-k       0.07      0.09      0.08       122\n",
      "        sager-e       0.36      0.05      0.09       488\n",
      "        saibi-e       0.00      0.00      0.00        12\n",
      "    salisbury-h       0.01      0.03      0.01        35\n",
      "      sanchez-m       0.00      0.03      0.01        32\n",
      "      sanders-r       0.43      0.18      0.26       619\n",
      "     scholtes-d       0.04      0.03      0.03        80\n",
      "  schoolcraft-d       0.13      0.19      0.15       167\n",
      "    schwieger-j       0.05      0.04      0.04        52\n",
      "        scott-s       0.41      0.07      0.12       779\n",
      "    semperger-c       0.03      0.03      0.03        80\n",
      "   shackleton-s       0.96      0.69      0.80      1256\n",
      "     shankman-j       0.33      0.17      0.22       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.26      0.15      0.19       208\n",
      "     skilling-j       0.38      0.30      0.34       182\n",
      "      slinger-r       0.00      0.00      0.00        13\n",
      "        smith-m       0.12      0.02      0.04       232\n",
      "      solberg-g       0.01      0.40      0.01        20\n",
      "        south-s       0.00      0.08      0.01        13\n",
      "        staab-t       0.05      0.17      0.07        23\n",
      "      stclair-c       0.77      0.69      0.73       415\n",
      "      steffes-j       0.50      0.35      0.41       392\n",
      " stepenovitch-j       0.01      0.47      0.03        19\n",
      "      stokley-c       0.27      0.33      0.30       170\n",
      "       storey-g       0.02      0.11      0.04        37\n",
      "        sturm-f       0.03      0.12      0.05       120\n",
      "     swerzbin-m       0.00      0.00      0.00        24\n",
      "        symes-k       1.00      0.74      0.85       748\n",
      "       taylor-m       0.55      0.15      0.24       714\n",
      "        tholt-j       0.00      0.00      0.00       198\n",
      "       thomas-p       0.00      0.00      0.00        55\n",
      "     townsend-j       0.01      0.15      0.01        27\n",
      "     tycholiz-b       0.00      0.00      0.00       163\n",
      "         ward-k       0.62      0.09      0.15       284\n",
      "       watson-k       0.55      0.19      0.28       277\n",
      "       weldon-c       0.03      0.01      0.02       136\n",
      "      whalley-g       0.01      0.09      0.02        33\n",
      "      whalley-l       0.07      0.16      0.09       119\n",
      "        white-s       0.23      0.10      0.14       146\n",
      "        whitt-m       0.13      0.15      0.14        79\n",
      "     williams-j       0.00      0.00      0.00        45\n",
      "    williams-w3       0.37      0.12      0.18       157\n",
      "        wolfe-j       0.01      0.04      0.01        24\n",
      "       ybarbo-p       0.04      0.10      0.05        31\n",
      "       zipper-a       0.00      0.00      0.00        91\n",
      "     zufferli-j       0.11      0.13      0.12       105\n",
      "\n",
      "       accuracy                           0.34     38054\n",
      "      macro avg       0.21      0.17      0.16     38054\n",
      "   weighted avg       0.57      0.34      0.40     38054\n",
      "\n",
      "Accuracy :  0.3366794555105902\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC trained on 88792 samples in 924.1644868850708s time with prediction time 602.6561586856842s .\n",
      "\n",
      "SVC Classification Report \n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        allen-p       0.74      0.48      0.58       473\n",
      "       arnold-j       0.55      0.25      0.34       727\n",
      "        arora-h       0.03      0.20      0.06        20\n",
      "       badeer-r       0.02      0.12      0.03        16\n",
      "       bailey-s       0.07      0.50      0.12         4\n",
      "         bass-e       0.85      0.54      0.66       893\n",
      "     baughman-d       0.01      0.03      0.01        32\n",
      "         beck-s       0.69      0.63      0.66       801\n",
      "       benson-r       0.03      0.25      0.05         8\n",
      "        blair-l       0.59      0.48      0.53       293\n",
      "      brawner-s       0.07      0.22      0.11       109\n",
      "          buy-r       0.35      0.35      0.35       233\n",
      "     campbell-l       0.15      0.20      0.17       203\n",
      "       carson-m       0.10      0.14      0.12       135\n",
      "         cash-m       0.58      0.45      0.50       325\n",
      "    causholli-m       0.21      0.41      0.28        69\n",
      "       corman-s       0.22      0.04      0.07       185\n",
      "     crandell-s       0.12      0.13      0.12        47\n",
      "       cuilla-m       0.04      0.08      0.05        53\n",
      "     dasovich-j       0.83      0.54      0.65      1609\n",
      "        davis-d       0.21      0.25      0.23       114\n",
      "         dean-c       0.01      0.08      0.02        13\n",
      "     delainey-d       0.61      0.60      0.60       526\n",
      "      derrick-j       0.25      0.38      0.30       204\n",
      "      dickson-s       0.02      0.12      0.04        56\n",
      "       donoho-l       0.09      0.12      0.10        64\n",
      "      donohoe-t       0.04      0.33      0.07        15\n",
      "      dorland-c       0.49      0.54      0.52       323\n",
      "        ermis-f       0.03      0.38      0.05        13\n",
      "       farmer-d       0.66      0.44      0.53       484\n",
      "      fischer-m       0.12      0.29      0.17        34\n",
      "       forney-j       0.14      0.10      0.12       115\n",
      "       fossum-d       0.80      0.67      0.73       645\n",
      "         gang-l       0.16      0.47      0.23        32\n",
      "          gay-r       0.08      0.14      0.10       181\n",
      "     geaccone-t       0.19      0.18      0.18       168\n",
      "      germany-c       0.91      0.34      0.50      1560\n",
      " gilbertsmith-d       0.00      0.00      0.00         3\n",
      "        giron-d       0.70      0.33      0.45       580\n",
      "     griffith-j       0.06      0.38      0.10        13\n",
      "      grigsby-m       0.47      0.64      0.54       260\n",
      "       guzman-m       0.05      0.22      0.08        96\n",
      "     haedicke-m       0.36      0.59      0.45       358\n",
      "         hain-m       0.18      0.46      0.26       123\n",
      "     hayslett-r       0.29      0.26      0.27       190\n",
      "        heard-m       0.57      0.61      0.59       255\n",
      "  hendrickson-s       0.05      0.30      0.09        33\n",
      "    hernandez-j       0.30      0.44      0.36       220\n",
      "        hodge-j       0.06      0.18      0.09        49\n",
      "        holst-k       0.01      0.25      0.03        12\n",
      "       horton-s       0.09      0.07      0.08       254\n",
      "        hyatt-k       0.07      0.04      0.05       101\n",
      "         hyvl-d       0.21      0.28      0.24       201\n",
      "        jones-t       0.91      0.51      0.65      1229\n",
      "     kaminski-v       0.98      0.85      0.91      2561\n",
      "         kean-s       0.29      0.12      0.17       544\n",
      "       keavey-p       0.11      0.19      0.14        58\n",
      "       keiser-k       0.30      0.52      0.38       119\n",
      "         king-j       0.00      0.00      0.00         2\n",
      "      kitchen-l       0.60      0.22      0.32       327\n",
      "   kuykendall-t       0.02      0.01      0.01       159\n",
      "     lavorato-j       0.33      0.14      0.19       547\n",
      "          lay-k       0.18      0.44      0.25       172\n",
      "      lenhart-m       0.33      0.15      0.20       828\n",
      "        lewis-a       0.03      0.16      0.06        38\n",
      "       linder-e       0.00      0.00      0.00         4\n",
      "        lokay-m       0.18      0.22      0.20        96\n",
      "        lokey-t       0.05      0.16      0.08        44\n",
      "         love-p       0.79      0.68      0.73       676\n",
      "        lucci-p       0.06      0.12      0.09        72\n",
      "        maggi-m       0.73      0.53      0.62        45\n",
      "         mann-k       0.98      0.79      0.87      2763\n",
      "       martin-t       0.11      0.10      0.11        68\n",
      "          may-l       0.03      0.06      0.04        35\n",
      "      mccarty-d       0.04      0.04      0.04        47\n",
      "    mcconnell-m       0.69      0.52      0.59       484\n",
      "        mckay-b       0.02      0.04      0.02        49\n",
      "        mckay-j       0.07      0.10      0.09        87\n",
      "   mclaughlin-e       0.23      0.24      0.24       212\n",
      "      merriss-s       0.00      0.00      0.00         0\n",
      "       meyers-a       0.00      0.00      0.00         2\n",
      "mims-thurston-p       0.05      0.04      0.04       207\n",
      "       motley-m       0.00      0.00      0.00         1\n",
      "         neal-s       0.34      0.21      0.26       330\n",
      "        nemec-g       0.47      0.26      0.34       621\n",
      "        panus-s       0.06      0.45      0.11        11\n",
      "        parks-j       0.27      0.24      0.25       164\n",
      "      pereira-s       0.21      0.39      0.27        99\n",
      "  perlingiere-d       0.96      0.82      0.89       704\n",
      "       phanis-s       0.00      0.00      0.00         3\n",
      "      pimenov-v       0.00      0.00      0.00        25\n",
      "      platter-p       0.07      0.13      0.09        31\n",
      "       presto-k       0.41      0.21      0.28       266\n",
      "       quenet-j       0.01      0.60      0.02        43\n",
      "      quigley-d       0.32      0.08      0.13       143\n",
      "         rapp-b       0.03      0.15      0.06        27\n",
      "    reitmeyer-j       0.00      0.00      0.00        18\n",
      "       richey-c       0.25      0.20      0.22       105\n",
      "         ring-a       0.13      0.27      0.17        84\n",
      "         ring-r       0.05      0.24      0.09        17\n",
      "     rodrique-r       0.43      0.33      0.37       451\n",
      "       rogers-b       0.87      0.71      0.78       550\n",
      "     ruscitti-k       0.21      0.25      0.23       122\n",
      "        sager-e       0.47      0.09      0.15       488\n",
      "        saibi-e       0.01      0.08      0.02        12\n",
      "    salisbury-h       0.03      0.14      0.05        35\n",
      "      sanchez-m       0.04      0.19      0.07        32\n",
      "      sanders-r       0.50      0.26      0.34       619\n",
      "     scholtes-d       0.02      0.03      0.02        80\n",
      "  schoolcraft-d       0.28      0.29      0.29       167\n",
      "    schwieger-j       0.08      0.21      0.12        52\n",
      "        scott-s       0.58      0.19      0.28       779\n",
      "    semperger-c       0.11      0.05      0.07        80\n",
      "   shackleton-s       0.95      0.82      0.88      1256\n",
      "     shankman-j       0.43      0.24      0.31       332\n",
      "      shapiro-r       0.00      0.00      0.00         2\n",
      "      shively-h       0.26      0.16      0.20       208\n",
      "     skilling-j       0.33      0.53      0.41       182\n",
      "      slinger-r       0.02      0.23      0.04        13\n",
      "        smith-m       0.16      0.07      0.10       232\n",
      "      solberg-g       0.00      0.05      0.00        20\n",
      "        south-s       0.02      0.46      0.04        13\n",
      "        staab-t       0.05      0.22      0.08        23\n",
      "      stclair-c       0.71      0.87      0.78       415\n",
      "      steffes-j       0.61      0.51      0.56       392\n",
      " stepenovitch-j       0.06      0.68      0.11        19\n",
      "      stokley-c       0.29      0.49      0.37       170\n",
      "       storey-g       0.03      0.08      0.04        37\n",
      "        sturm-f       0.07      0.05      0.06       120\n",
      "     swerzbin-m       0.10      0.21      0.13        24\n",
      "        symes-k       0.98      0.83      0.90       748\n",
      "       taylor-m       0.62      0.28      0.38       714\n",
      "        tholt-j       0.10      0.02      0.03       198\n",
      "       thomas-p       0.08      0.04      0.05        55\n",
      "     townsend-j       0.04      0.33      0.07        27\n",
      "     tycholiz-b       0.29      0.05      0.08       163\n",
      "         ward-k       0.58      0.31      0.40       284\n",
      "       watson-k       0.58      0.36      0.45       277\n",
      "       weldon-c       0.12      0.02      0.04       136\n",
      "      whalley-g       0.03      0.06      0.04        33\n",
      "      whalley-l       0.12      0.11      0.11       119\n",
      "        white-s       0.40      0.26      0.32       146\n",
      "        whitt-m       0.17      0.37      0.23        79\n",
      "     williams-j       0.09      0.09      0.09        45\n",
      "    williams-w3       0.49      0.20      0.29       157\n",
      "        wolfe-j       0.03      0.17      0.05        24\n",
      "       ybarbo-p       0.07      0.19      0.10        31\n",
      "       zipper-a       0.01      0.05      0.02        91\n",
      "     zufferli-j       0.11      0.25      0.15       105\n",
      "\n",
      "       accuracy                           0.45     38054\n",
      "      macro avg       0.26      0.27      0.23     38054\n",
      "   weighted avg       0.61      0.45      0.50     38054\n",
      "\n",
      "Accuracy :  0.45219950596520736\n",
      "******************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for clf in [clf_A]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    \n",
    "    for i in samples.values():\n",
    "        results[clf_name][i] = train_predict(clf, i, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Voting Ensembling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5401613560288524\n",
      "Wall time: 3h 30min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression(multi_class='auto' , random_state=seed)\n",
    "estimators.append(('logistic', model1))\n",
    "\n",
    "model2 = tree.DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "model3 = svm.SVC(kernel='linear' , class_weight = 'balanced')\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X_train, y_train, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53558559, 0.53761261, 0.53598378, 0.53823629, 0.54161505,\n",
       "       0.54893569, 0.53023989, 0.54398018, 0.54105192, 0.54837256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Stacking enembling technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
