{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Innoplexus Online Hiring Hackathon: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://datahack.analyticsvidhya.com/contest/innoplexus-online-hiring-hackathon/\n",
    "- https://www.machinelearningplus.com/nlp/gensim-tutorial/  *Check out this tutorial on gensim*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for drugs/medicines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays the narrative of a brand is not only built and controlled by the company that owns the brand. For this reason, companies are constantly looking out across Blogs, Forums, and other social media platforms, etc for checking the sentiment for their various products and also competitor products to learn how their brand resonates in the market. This kind of analysis helps them as part of their post-launch market research. This is relevant for a lot of industries including pharma and their drugs.\n",
    " \n",
    "\n",
    "The challenge is that the language used in this type of content is not strictly grammatically correct. Some use sarcasm. Others cover several topics with different sentiments in one post. Other users post comments and reply and thereby indicating his/her sentiment around the topic.\n",
    "\n",
    "Sentiment can be clubbed into 3 major buckets - Positive, Negative and Neutral Sentiments.\n",
    "\n",
    " \n",
    "\n",
    "You are provided with data containing samples of text. This text can contain one or more drug mentions. Each row contains a unique combination of the text and the drug mention. Note that the same text can also have different sentiment for a different drug.\n",
    "\n",
    "Given the text and drug name, the task is to predict the sentiment for texts contained in the test dataset. Given below is an example of text from the dataset:\n",
    "\n",
    " \n",
    "\n",
    "Example:\n",
    "\n",
    "*Stelara is still fairly new to Crohn's treatment. This is why you might not get a lot of replies. I've done some research, but most of the \"time to work\" answers are from Psoriasis boards. For Psoriasis, it seems to be about 4-12 weeks to reach a strong therapeutic level. The good news is, Stelara seems to be getting rave reviews from Crohn's patients. It seems to be the best med to come along since Remicade. I hope you have good success with it. My daughter was diagnosed Feb. 19/07, (13 yrs. old at the time of diagnosis), with Crohn's of the Terminal Illium. Has used Prednisone and Pentasa. Started Imuran (02/09), had an abdominal abscess (12/08). 2cm of Stricture. Started ​Remicade in Feb. 2014, along with 100mgs. of Imuran.*\n",
    "\n",
    "For Stelara the above text is ​positive​ while for Remicade the above text is ​negative​."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TRAIN DATA*  \n",
    "unique_hash - \tUnique ID  \n",
    "text\t    -   text pertaining to the drugs  \n",
    "drug\t    -   drug name for which the sentiment is provided  \n",
    "sentiment\t-   (Target) 0-positive, 1-negative, 2-neutral  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THINGS TO DO FOR IMPROVING MODEL\n",
    "\n",
    "*2 sentiment remove few records  \n",
    "fasttext  \n",
    "drug column...feature add 200, train word 2 vec  \n",
    "random forrest  \n",
    "gbbost  \n",
    "deep learning -lstm  \n",
    "hoole house andrew ng  \n",
    "analysis on sentiment - give weightage....multiply buy 1.5 every weight*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pca  \n",
    "tfidf change  \n",
    "ensemble  \n",
    " predictprobba 3 class probability -  1 row  \n",
    " append all 3   \n",
    " y train shape same  \n",
    " x_Train 9 cols  \n",
    " logistic regression new dataset on 10   \n",
    " test data same process*  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more common imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# languange processing imports\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#model for vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#model for splitting of data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#logistic regression algo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "#svm algo \n",
    "from sklearn import svm\n",
    "\n",
    "# Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set()  # defines the style of the plots to be seaborn style\n",
    "\n",
    "##pretty print\n",
    "import pprint as pp\n",
    "\n",
    "#punctuation strings\n",
    "import string\n",
    "\n",
    "#logging each run of cells\n",
    "#import logging\n",
    "\n",
    "#use multi cores\n",
    "import multiprocessing\n",
    "\n",
    "#calculation of time\n",
    "from time import time\n",
    "\n",
    "#accuracy of model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#SMOTE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5279, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. As for Gilenya – if you feel good, don’t think about it, it won’t change anything but waste your time and energy. I’m taking Tysabri and feel amazing, no symptoms (other than dodgy color vision, but I’ve had it since always, so, don’t know) and I don’t know if it will last a month, a year, a decade, ive just decided to enjoy the ride, no point in worrying.</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why you’d want to try it. But, results reported in lectures don’t always stand up to the scrutiny of peer-review during publication. There so much still to do before this is convincing. I hope that it does work out, I really do. And if you’re aware of and happy with the risks, then that’s great. I just think it’s important to present this in a balanced way, and to understand why we don’t move straight from the first show of promise in an animal study to using drugs on humans. There’s still a lot of animal data to gather, and human data to gather before anyone can tell if it’s safe or effective. I can’t tell you how many times animal studies don’t follow through to humans, but it’s one of the major attrition points in drug development. You’ve been through some of the unpredictability issues with Cladribine/Gilenya, where there was an interaction that wasn’t predicted. But once people try it, the doctors can see patterns and work out what’s going on. Clemastine/metformin is very exciting, and given what you’ve said about your current condition and your personal risk tolerance it makes sense to try it. It definitely wouldn’t be for everyone.</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 receptors rather than 1-5 like Fingolimod. Hoping to soon see what the AEs and SAEs were Yes. I'm not sure what this means, exactly:  Quote Nine patients reported serious adverse events (2 mg: 3/29 [10.3%], 1.25 mg: 1/43 [2.3%], 0.5 mg: 4/29 [13.8%], and 0.25 mg: 1/50 [2.0%]; no serious adverse event was reported for more than 1 patient and no new safety signals occurred compared with the BOLD Study. If there were 9 patients reporting SAEs, how can it be stated that \"no serious adverse event was reported for more than 1 patient...\"? Maybe I haven't read this right, or maybe there's a misprint. I'm very pleased that something is being developed for SPMS, and it's encouraging that siponimod doesn't linger for very long in the body.</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder where lemtrada and ocrevus sales would go, if they prove anti-cd20 are induction</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain and Cervical Cord are in and my next Neurologist appointment is in the next couple of weeks. There’re no new lesions in Brain/Cord and I’ve had no relapses while I was on Gilenya. This was a good sign. But there was one line in the cervical cord review that concerned me. It goes : “Lesions at C2-3 and T2 now show hypointensity on the post gadolinium T1 images only. This could represent artifact or early axonal loss.” That was bothersome to read. What are the kind of symptoms from C2-C3 lesion should I be aware of ? Would it result in change of my DMT ? Thanks.</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "0  Autoimmune diseases tend to come in clusters. As for Gilenya – if you feel good, don’t think about it, it won’t change anything but waste your time and energy. I’m taking Tysabri and feel amazing, no symptoms (other than dodgy color vision, but I’ve had it since always, so, don’t know) and I don’t know if it will last a month, a year, a decade, ive just decided to enjoy the ride, no point in worrying.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1  I can completely understand why you’d want to try it. But, results reported in lectures don’t always stand up to the scrutiny of peer-review during publication. There so much still to do before this is convincing. I hope that it does work out, I really do. And if you’re aware of and happy with the risks, then that’s great. I just think it’s important to present this in a balanced way, and to understand why we don’t move straight from the first show of promise in an animal study to using drugs on humans. There’s still a lot of animal data to gather, and human data to gather before anyone can tell if it’s safe or effective. I can’t tell you how many times animal studies don’t follow through to humans, but it’s one of the major attrition points in drug development. You’ve been through some of the unpredictability issues with Cladribine/Gilenya, where there was an interaction that wasn’t predicted. But once people try it, the doctors can see patterns and work out what’s going on. Clemastine/metformin is very exciting, and given what you’ve said about your current condition and your personal risk tolerance it makes sense to try it. It definitely wouldn’t be for everyone.   \n",
       "2  Interesting that it only targets S1P-1/5 receptors rather than 1-5 like Fingolimod. Hoping to soon see what the AEs and SAEs were Yes. I'm not sure what this means, exactly:  Quote Nine patients reported serious adverse events (2 mg: 3/29 [10.3%], 1.25 mg: 1/43 [2.3%], 0.5 mg: 4/29 [13.8%], and 0.25 mg: 1/50 [2.0%]; no serious adverse event was reported for more than 1 patient and no new safety signals occurred compared with the BOLD Study. If there were 9 patients reporting SAEs, how can it be stated that \"no serious adverse event was reported for more than 1 patient...\"? Maybe I haven't read this right, or maybe there's a misprint. I'm very pleased that something is being developed for SPMS, and it's encouraging that siponimod doesn't linger for very long in the body.                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3  Very interesting, grand merci. Now I wonder where lemtrada and ocrevus sales would go, if they prove anti-cd20 are induction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4  Hi everybody, My latest MRI results for Brain and Cervical Cord are in and my next Neurologist appointment is in the next couple of weeks. There’re no new lesions in Brain/Cord and I’ve had no relapses while I was on Gilenya. This was a good sign. But there was one line in the cervical cord review that concerned me. It goes : “Lesions at C2-3 and T2 now show hypointensity on the post gadolinium T1 images only. This could represent artifact or early axonal loss.” That was bothersome to read. What are the kind of symptoms from C2-C3 lesion should I be aware of ? Would it result in change of my DMT ? Thanks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "         drug  sentiment  \n",
       "0  gilenya     2          \n",
       "1  gilenya     2          \n",
       "2  fingolimod  2          \n",
       "3  ocrevus     2          \n",
       "4  gilenya     1          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2924, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
       "      <td>256 (previously stable on natalizumab), with 55% switching to fingolimod</td>\n",
       "      <td>fingolimod</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  9e9a8166b84114aca147bf409f6f956635034c08   \n",
       "\n",
       "                                                                       text  \\\n",
       "0  256 (previously stable on natalizumab), with 55% switching to fingolimod   \n",
       "\n",
       "         drug  \n",
       "0  fingolimod  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "print(test_data.shape)\n",
    "test_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Sentiment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3825\n",
       "1    837 \n",
       "0    617 \n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.drug.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train_test_list = [ train_data, test_data ]\n",
    "\n",
    "#here ignore_index True will create new values from 0 to n-1\n",
    "train_test_data = pd.concat(train_test_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8203, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    \n",
    "    #convert text to lower-case\n",
    "    data['tokenized_text'] = data['text'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
    "\n",
    "    #remove punctuations, unwanted characters\n",
    "    data['tokenized_text_1']= data['tokenized_text'].apply(lambda x: \"\".join([char for char in x if char not in string.punctuation]))\n",
    "\n",
    "    #remove numbers\n",
    "    data['tokenized_text_2']= data['tokenized_text_1'].apply(lambda x: re.sub('[0-9]+', ' ' , x))\n",
    "\n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    data['tokenized_text_3']= data['tokenized_text_2'].apply(lambda x:' '.join(x for x in x.split() if not x in stop))\n",
    "\n",
    "    #lemmatization\n",
    "    data['tokenized_text_4']= data['tokenized_text_3'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "    # remove all single characters\n",
    "    data['tokenized_text_5'] = data['tokenized_text_4'].apply(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "    \n",
    "    #create a final text field to work on\n",
    "    data['final_text'] = data['tokenized_text_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing tfidf\n",
    "1. # max_df = ignore terms that appear in more than 50% of the documents\n",
    "\n",
    "2. # min_df = only keep terms that appear in at least 2 documents\n",
    "\n",
    "#          If float, the parameter represents a proportion of documents.\n",
    "#          If integer, the parameter represents an absolute count.          \n",
    "\n",
    "3. # # ngram_range # #\n",
    "# Example: range(1,2)\n",
    "# v=text.CountVectorizer(ngramrange=(1,2)) print(v.fit([\"an apple a day keeps the doctor away\"]).vocabulary)\n",
    "\n",
    "# Result:\n",
    "# {'an': 0, 'apple': 2, 'day': 5, 'keeps': 9, 'the': 11, 'doctor': 7, 'away': 4, 'an apple': 1, 'apple day': 3, 'day keeps': 6, 'keeps the': 10, 'the doctor': 12, 'doctor away': 8}\n",
    "\n",
    "# Example: range(1,3)\n",
    "# v=text.CountVectorizer(ngramrange=(1,3)) print(v.fit([\"an apple a day keeps the doctor away\"]).vocabulary)\n",
    "\n",
    "# Result:\n",
    "# {'an': 0, 'apple': 3, 'day': 7, 'keeps': 12, 'the': 15, 'doctor': 10, 'away': 6, 'an apple': 1, 'apple day': 4, 'day keeps': 8, 'keeps the': 13, 'the doctor': 16, 'doctor away': 11, 'an apple day': 2, 'apple day keeps': 5, 'day keeps the': 9, 'keeps the doctor': 14, 'the doctor away': 17}\n",
    "\n",
    "\n",
    "def vectorization(data):\n",
    "\n",
    "#     tf=TfidfVectorizer( min_df=10 \n",
    "#                        , max_df=0.7\n",
    "#                        , lowercase=True \n",
    "#                        , ngram_range=(0, 2) )\n",
    "    t0 = time()\n",
    "    tf=TfidfVectorizer( min_df=10  , ngram_range=(0, 3))\n",
    "\n",
    "    tfidf_dtm = tf.fit_transform(data).toarray()\n",
    "    print('Shape of document term matrix : ', tfidf_dtm.shape)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    return list(tfidf_dtm) , tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the drug column\n",
    "\n",
    "def merge_drug_tfidf(data):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(data.drug)\n",
    "    data['drug_label'] = label_encoder.transform(data.drug)\n",
    "\n",
    "    # here we add the drug column label to the tfidf vector\n",
    "\n",
    "    ls = []\n",
    "    for key,value in data.iterrows():\n",
    "        ls.append(np.append(value['tfidf'],value['drug_label']))\n",
    "\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pre-processing or cleaning data\n",
    "\n",
    "text_preprocessing(train_data)\n",
    "#text_preprocessing(train_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document term matrix :  (5279, 16465)\n",
      "done in 10.765s\n"
     ]
    }
   ],
   "source": [
    "#build a vector from the txt data\n",
    "\n",
    "train_data['tfidf'] , tf = vectorization(train_data['final_text'])\n",
    "#train_test_data['tfidf'] , tf = vectorization(train_test_data['final_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the drug column with the tfidf vector, append at last\n",
    "\n",
    "train_data['final'] = merge_drug_tfidf(train_data)\n",
    "#train_test_data['final'] = merge_drug_tfidf(train_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_hash         object\n",
       "text                object\n",
       "drug                object\n",
       "sentiment           int64 \n",
       "tokenized_text      object\n",
       "tokenized_text_1    object\n",
       "tokenized_text_2    object\n",
       "tokenized_text_3    object\n",
       "tokenized_text_4    object\n",
       "tokenized_text_5    object\n",
       "final_text          object\n",
       "tfidf               object\n",
       "drug_label          int32 \n",
       "final               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train_data['final'].to_numpy\n",
    "# type(X)\n",
    "\n",
    "# # output -method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = np.array(list(map(np.array, train_data['final'])))\n",
    "# type(X1)\n",
    "\n",
    "# # output - numpy.ndarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, label, test_size):\n",
    "    \n",
    "    X = np.array(list(map(np.array, data)))\n",
    "    y = label\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=test_size , random_state=42)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3695, 16466)\n",
      "(3695,)\n",
      "(1584, 16466)\n",
      "(1584,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test  = split_data(train_data['final'] , train_data['sentiment'] , test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE  \n",
    "Always split into test and train sets BEFORE trying oversampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '2': 2683\n",
      "Before OverSampling, counts of label '1': 568\n",
      "Before OverSampling, counts of label '0': 444 \n",
      "\n",
      "After OverSampling, the shape of train_X: (8049, 16466)\n",
      "After OverSampling, the shape of train_y: (8049,) \n",
      "\n",
      "After OverSampling, counts of label '1': 2683\n",
      "After OverSampling, counts of label '1': 2683\n",
      "After OverSampling, counts of label '0': 2683\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '2': {}\".format(sum(y_train==2)))\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(random_state=42 , sampling_strategy='auto')\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==2)))\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classsification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initiaize supervised learning models\n",
    "\n",
    "def data_sampling(y_train):\n",
    "    \n",
    "    #  Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "    #  samples_100 is the entire training set i.e. len(y_train)\n",
    "    #  samples_10 is 10% of samples_100\n",
    "    #  samples_1 is 1% of samples_100 \n",
    "    \n",
    "    samples = {}\n",
    "    \n",
    "    samples['samples_1']   = int(len(y_train)/100)\n",
    "    samples['samples_10']  = int(len(y_train)*10/100)\n",
    "    samples['samples_100'] = len(y_train)\n",
    "    \n",
    "\n",
    "    print('samples_1   : ', samples['samples_1'])\n",
    "    print('samples_10  : ', samples['samples_10'])\n",
    "    print('samples_100 : ', samples['samples_100'])\n",
    "   \n",
    "    return samples\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict and Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(learner, sample_size, X_train, y_train, X_val, y_val): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_val: features validation set\n",
    "       - y_val: income validation set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train[:sample_size],y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end-start\n",
    "        \n",
    "    # Get the predictions on the validation set(X_val),\n",
    "    start = time() # Get start time\n",
    "    predictions_val = learner.predict(X_val)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "        \n",
    "    #Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_val,predictions_val)\n",
    "    \n",
    "    #classification report for precision, relcall, f1-score\n",
    "    results['f_val'] = classification_report(y_val, predictions_val, target_names=target_names)\n",
    "               \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples in {} time with prediction time {} .\".format(learner.__class__.__name__\n",
    "                                                                                , sample_size,results['train_time'] \n",
    "                                                                                , results['pred_time'] ))\n",
    "    print()\n",
    "    print(\"{} Classification Report \".format(learner.__class__.__name__) )\n",
    "    print()\n",
    "    print(results['f_val'] )\n",
    "    print(\"Accuracy : \", results['acc_test'])\n",
    "    print(\"******************************************************************************************************************\")\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_1   :  80\n",
      "samples_10  :  804\n",
      "samples_100 :  8049\n",
      "\n",
      "{'samples_1': 80, 'samples_10': 804, 'samples_100': 8049}\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_res\n",
    "y_train = y_train_res\n",
    "\n",
    "samples = {}\n",
    "\n",
    "samples = data_sampling(y_train_res)\n",
    "\n",
    "#sentiment target names\n",
    "\n",
    "target_names = ['0', '1' , '2' ]\n",
    "\n",
    "print()\n",
    "pp.pprint(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "clf_A = svm.SVC(kernel='linear')\n",
    "clf_B = LogisticRegression(multi_class='auto' , random_state=42)\n",
    "clf_C = MultinomialNB()\n",
    "#clf_D = XGBClassifier()\n",
    "\n",
    "print()\n",
    "pp.pprint(clf_A)\n",
    "print()\n",
    "pp.pprint(clf_B)\n",
    "print()\n",
    "pp.pprint(clf_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 80 samples in 0.021971464157104492 time with prediction time 0.08374571800231934 .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7209595959595959\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression trained on 804 samples in 0.15860342979431152 time with prediction time 0.07277894020080566 .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7209595959595959\n",
      "******************************************************************************************************************\n",
      "LogisticRegression trained on 8049 samples in 2.868330955505371 time with prediction time 0.06483006477355957 .\n",
      "\n",
      "LogisticRegression Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.39      0.28       173\n",
      "           1       0.33      0.52      0.41       269\n",
      "           2       0.80      0.60      0.69      1142\n",
      "\n",
      "    accuracy                           0.57      1584\n",
      "   macro avg       0.45      0.50      0.46      1584\n",
      "weighted avg       0.66      0.57      0.60      1584\n",
      "\n",
      "Accuracy :  0.5650252525252525\n",
      "******************************************************************************************************************\n",
      "MultinomialNB trained on 80 samples in 0.006981849670410156 time with prediction time 0.06781959533691406 .\n",
      "\n",
      "MultinomialNB Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7209595959595959\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB trained on 804 samples in 0.0578460693359375 time with prediction time 0.06582474708557129 .\n",
      "\n",
      "MultinomialNB Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7196969696969697\n",
      "******************************************************************************************************************\n",
      "MultinomialNB trained on 8049 samples in 0.5654878616333008 time with prediction time 0.06437468528747559 .\n",
      "\n",
      "MultinomialNB Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.69      0.23       173\n",
      "           1       0.34      0.51      0.41       269\n",
      "           2       0.86      0.25      0.39      1142\n",
      "\n",
      "    accuracy                           0.34      1584\n",
      "   macro avg       0.45      0.48      0.34      1584\n",
      "weighted avg       0.70      0.34      0.38      1584\n",
      "\n",
      "Accuracy :  0.3440656565656566\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC trained on 80 samples in 0.62432861328125 time with prediction time 3.7121381759643555 .\n",
      "\n",
      "SVC Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7209595959595959\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC trained on 804 samples in 55.53720283508301 time with prediction time 39.974427938461304 .\n",
      "\n",
      "SVC Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7209595959595959\n",
      "******************************************************************************************************************\n",
      "SVC trained on 8049 samples in 2401.645114183426 time with prediction time 304.62592101097107 .\n",
      "\n",
      "SVC Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.40      0.29       173\n",
      "           1       0.37      0.50      0.42       269\n",
      "           2       0.81      0.65      0.72      1142\n",
      "\n",
      "    accuracy                           0.60      1584\n",
      "   macro avg       0.47      0.52      0.48      1584\n",
      "weighted avg       0.67      0.60      0.62      1584\n",
      "\n",
      "Accuracy :  0.5978535353535354\n",
      "******************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for clf in [clf_B,clf_C,clf_A ]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    \n",
    "    for i in samples.values():\n",
    "        results[clf_name][i] = train_predict(clf, i, X_train, y_train, X_test, y_test)\n",
    "\n",
    "#     for i, samples in enumerate([samples_1 , samples_10, samples_100]):\n",
    "#         results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier trained on 80 samples in 0.12808942794799805 time with prediction time 0.1720106601715088 .\n",
      "\n",
      "RandomForestClassifier Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7209595959595959\n",
      "******************************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshrs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier trained on 804 samples in 0.40399932861328125 time with prediction time 0.1400313377380371 .\n",
      "\n",
      "RandomForestClassifier Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       173\n",
      "           1       0.00      0.00      0.00       269\n",
      "           2       0.72      1.00      0.84      1142\n",
      "\n",
      "    accuracy                           0.72      1584\n",
      "   macro avg       0.24      0.33      0.28      1584\n",
      "weighted avg       0.52      0.72      0.60      1584\n",
      "\n",
      "Accuracy :  0.7209595959595959\n",
      "******************************************************************************************************************\n",
      "RandomForestClassifier trained on 8049 samples in 3.868272304534912 time with prediction time 0.15199756622314453 .\n",
      "\n",
      "RandomForestClassifier Classification Report \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.06      0.08       173\n",
      "           1       0.27      0.10      0.15       269\n",
      "           2       0.72      0.88      0.79      1142\n",
      "\n",
      "    accuracy                           0.66      1584\n",
      "   macro avg       0.37      0.35      0.34      1584\n",
      "weighted avg       0.58      0.66      0.61      1584\n",
      "\n",
      "Accuracy :  0.6590909090909091\n",
      "******************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_D = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "\n",
    "results = {}\n",
    "for clf in [clf_D]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    \n",
    "    for i in samples.values():\n",
    "        results[clf_name][i] = train_predict(clf, i, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training on full train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4223, 13952)\n",
      "(1056, 13952)\n",
      "(4223,)\n",
      "(1056,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(train_data['final'] , train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the SVM test data :  2069.0106766223907\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "trainer_model_svm = clf_A.fit(X_train,y_train)\n",
    "print(\"Time taken to train the SVM test data : \", time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the Logistic Regresssion test data :  0.8393814563751221\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "trainer_model_lr  = clf_B.fit(X_train,y_train)\n",
    "print(\"Time taken to train the Logistic Regresssion test data : \", time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we do the same text processing as done for training data\n",
    "\n",
    "text_preprocessing(test_data) #formats and cleans the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2924, 13951)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data['tfidf'] = list(tf.transform(test_data['final_text']).toarray())\n",
    "tf.transform(test_data['final_text']).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the drug column\n",
    "\n",
    "test_data['final'] = merge_drug_tfidf(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a numpy array for prediction\n",
    "\n",
    "X_test        = np.array(list(map(np.array, test_data['final'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict from the choosen model\n",
    "\n",
    "submission_predictions_svm = trainer_model_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_predictions_lr = trainer_model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2924, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b37d169dee5bdae27060949242fb54feb6a7f7f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  sentiment\n",
       "0  9e9a8166b84114aca147bf409f6f956635034c08  0        \n",
       "1  e747e6822c867571afe7b907b51f0f2ca67b0e1a  0        \n",
       "2  50b6d851bcff4f35afe354937949e9948975adf7  0        \n",
       "3  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae  0        \n",
       "4  8b37d169dee5bdae27060949242fb54feb6a7f7f  0        "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_csv('sample_submission.csv')\n",
    "print(sample_sub.shape)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           unique_hash\n",
       "sentiment             \n",
       "0          436        \n",
       "1          542        \n",
       "2          1946       "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"unique_hash\": test_data[\"unique_hash\"],\n",
    "        \"sentiment\": submission_predictions_svm\n",
    "    })\n",
    "submission.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission6_svm.csv', index=False)\n",
    "#0.58289"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
